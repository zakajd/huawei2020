{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T18:07:35.534005Z",
     "start_time": "2020-09-09T18:07:35.531570Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T07:51:50.412509Z",
     "start_time": "2020-09-20T07:51:48.886892Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T07:51:50.440254Z",
     "start_time": "2020-09-20T07:51:50.414340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "# Fix to be able to import python modules inside a notebook\n",
    "os.chdir('..')\n",
    "\n",
    "# Useful extensions\n",
    "# %load_ext watermark\n",
    "# %watermark -v -n -m -p numpy,torch,albumentations,photosynthesis_metrics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Nice plot formating\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T07:51:50.612679Z",
     "start_time": "2020-09-20T07:51:50.442483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs  Makefile    preprocess.py  requirements.txt  train.py\r\n",
      "data\t notebooks   __pycache__    src\r\n",
      "logs\t predict.py  README.md\t    tmp.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T12:32:48.032504Z",
     "start_time": "2020-09-19T12:32:48.030013Z"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T12:46:35.141574Z",
     "start_time": "2020-09-19T12:46:35.130514Z"
    }
   },
   "outputs": [],
   "source": [
    "\" knn module, all credits to faiss! \"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BaseKNN(object):\n",
    "    \"\"\"kNN  class\n",
    "    Args:\n",
    "        database: feature vectors in database. Shape (n_emb x emb_dim)\n",
    "        distance: Distance metric, one of {'cosine', 'euclidean'}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, database, method):\n",
    "        if database.dtype != np.float32:\n",
    "            database = database.astype(np.float32)\n",
    "        self.N = len(database)\n",
    "        self.D = database[0].shape[-1]\n",
    "        self.database = database if database.flags['C_CONTIGUOUS'] \\\n",
    "                               else np.ascontiguousarray(database)\n",
    "\n",
    "        self.index = {\n",
    "            'cosine': faiss.IndexFlatIP,\n",
    "            'euclidean': faiss.IndexFlatL2\n",
    "        }[method](self.D)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
    "            self.index = faiss.index_cpu_to_all_gpus(self.index)\n",
    "        self.add()\n",
    "\n",
    "    def add(self, batch_size=10000):\n",
    "        \"\"\"Add data into index\"\"\"\n",
    "        if self.N <= batch_size:\n",
    "            self.index.add(self.database)\n",
    "        else:\n",
    "            [self.index.add(self.database[i:i+batch_size])\n",
    "                    for i in tqdm(range(0, len(self.database), batch_size),\n",
    "                                  desc='[index] add')]\n",
    "\n",
    "    def search(self, queries, k):\n",
    "        \"\"\"Search\n",
    "        Args:\n",
    "            queries: query vectors\n",
    "            k: get top-k results\n",
    "        Returns:\n",
    "            sims: similarities of k-NN\n",
    "            ids: indexes of k-NN\n",
    "        \"\"\"\n",
    "        if not queries.flags['C_CONTIGUOUS']:\n",
    "            queries = np.ascontiguousarray(queries)\n",
    "        if queries.dtype != np.float32:\n",
    "            queries = queries.astype(np.float32)\n",
    "        sims, ids = self.index.search(queries, k)\n",
    "        return sims, ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T12:46:36.290897Z",
     "start_time": "2020-09-19T12:46:36.285500Z"
    }
   },
   "outputs": [],
   "source": [
    "\" dataset module \"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    \"\"\"Load features\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception(\"{} does not exist\".format(path))\n",
    "    ext = os.path.splitext(path)[-1]\n",
    "    return {'.npy': np, '.jbl': joblib}[ext].load(path)\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"Dataset class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query_path, gallery_path):\n",
    "        self.query_path = query_path\n",
    "        self.gallery_path = gallery_path\n",
    "        self._queries = None\n",
    "        self._gallery = None\n",
    "\n",
    "    @property\n",
    "    def queries(self):\n",
    "        if self._queries is None:\n",
    "            self._queries = load(self.query_path)\n",
    "        return self._queries\n",
    "\n",
    "    @property\n",
    "    def gallery(self):\n",
    "        if self._gallery is None:\n",
    "            self._gallery = load(self.gallery_path)\n",
    "        return self._gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T12:46:46.168382Z",
     "start_time": "2020-09-19T12:46:46.068740Z"
    }
   },
   "outputs": [],
   "source": [
    "\" diffusion module \"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from knn import KNN, ANN\n",
    "\n",
    "\n",
    "trunc_ids = None\n",
    "trunc_init = None\n",
    "lap_alpha = None\n",
    "\n",
    "\n",
    "def get_offline_result(i):\n",
    "    ids = trunc_ids[i]\n",
    "    trunc_lap = lap_alpha[ids][:, ids]\n",
    "    scores, _ = linalg.cg(trunc_lap, trunc_init, tol=1e-6, maxiter=20)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cache(filename):\n",
    "    \"\"\"Decorator to cache results\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kw):\n",
    "            self = args[0]\n",
    "            path = os.path.join(self.cache_dir, filename)\n",
    "            time0 = time.time()\n",
    "            if os.path.exists(path):\n",
    "                result = joblib.load(path)\n",
    "                cost = time.time() - time0\n",
    "                print('[cache] loading {} costs {:.2f}s'.format(path, cost))\n",
    "                return result\n",
    "            result = func(*args, **kw)\n",
    "            cost = time.time() - time0\n",
    "            print('[cache] obtaining {} costs {:.2f}s'.format(path, cost))\n",
    "            joblib.dump(result, path)\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Diffusion(object):\n",
    "    \"\"\"Diffusion class\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cache_dir):\n",
    "        self.features = features\n",
    "        self.N = len(self.features)\n",
    "        self.cache_dir = cache_dir\n",
    "        # use ANN for large datasets\n",
    "        self.use_ann = self.N >= 100000\n",
    "        if self.use_ann:\n",
    "            self.ann = ANN(self.features, method='cosine')\n",
    "        self.knn = KNN(self.features, method='cosine')\n",
    "\n",
    "    @cache('offline.jbl')\n",
    "    def get_offline_results(self, n_trunc, kd=50):\n",
    "        \"\"\"Get offline diffusion results for each gallery feature\n",
    "        \"\"\"\n",
    "        print('[offline] starting offline diffusion')\n",
    "        print('[offline] 1) prepare Laplacian and initial state')\n",
    "        global trunc_ids, trunc_init, lap_alpha\n",
    "        if self.use_ann:\n",
    "            _, trunc_ids = self.ann.search(self.features, n_trunc)\n",
    "            sims, ids = self.knn.search(self.features, kd)\n",
    "            lap_alpha = self.get_laplacian(sims, ids)\n",
    "        else:\n",
    "            sims, ids = self.knn.search(self.features, n_trunc)\n",
    "            trunc_ids = ids\n",
    "            lap_alpha = self.get_laplacian(sims[:, :kd], ids[:, :kd])\n",
    "        trunc_init = np.zeros(n_trunc)\n",
    "        trunc_init[0] = 1\n",
    "\n",
    "        print('[offline] 2) gallery-side diffusion')\n",
    "        results = Parallel(n_jobs=-1, prefer='threads')(delayed(get_offline_result)(i)\n",
    "                                      for i in tqdm(range(self.N),\n",
    "                                                    desc='[offline] diffusion'))\n",
    "        all_scores = np.concatenate(results)\n",
    "\n",
    "        print('[offline] 3) merge offline results')\n",
    "        rows = np.repeat(np.arange(self.N), n_trunc)\n",
    "        offline = sparse.csr_matrix((all_scores, (rows, trunc_ids.reshape(-1))),\n",
    "                                    shape=(self.N, self.N),\n",
    "                                    dtype=np.float32)\n",
    "        return offline\n",
    "\n",
    "    # @cache('laplacian.jbl')\n",
    "    def get_laplacian(self, sims, ids, alpha=0.99):\n",
    "        \"\"\"Get Laplacian_alpha matrix\n",
    "        \"\"\"\n",
    "        affinity = self.get_affinity(sims, ids)\n",
    "        num = affinity.shape[0]\n",
    "        degrees = affinity @ np.ones(num) + 1e-12\n",
    "        # mat: degree matrix ^ (-1/2)\n",
    "        mat = sparse.dia_matrix(\n",
    "            (degrees ** (-0.5), [0]), shape=(num, num), dtype=np.float32)\n",
    "        stochastic = mat @ affinity @ mat\n",
    "        sparse_eye = sparse.dia_matrix(\n",
    "            (np.ones(num), [0]), shape=(num, num), dtype=np.float32)\n",
    "        lap_alpha = sparse_eye - alpha * stochastic\n",
    "        return lap_alpha\n",
    "\n",
    "    # @cache('affinity.jbl')\n",
    "    def get_affinity(self, sims, ids, gamma=3):\n",
    "        \"\"\"Create affinity matrix for the mutual kNN graph of the whole dataset\n",
    "        Args:\n",
    "            sims: similarities of kNN\n",
    "            ids: indexes of kNN\n",
    "        Returns:\n",
    "            affinity: affinity matrix\n",
    "        \"\"\"\n",
    "        num = sims.shape[0]\n",
    "        sims[sims < 0] = 0  # similarity should be non-negative\n",
    "        sims = sims ** gamma\n",
    "        # vec_ids: feature vectors' ids\n",
    "        # mut_ids: mutual (reciprocal) nearest neighbors' ids\n",
    "        # mut_sims: similarites between feature vectors and their mutual nearest neighbors\n",
    "        vec_ids, mut_ids, mut_sims = [], [], []\n",
    "        for i in range(num):\n",
    "            # check reciprocity: i is in j's kNN and j is in i's kNN when i != j\n",
    "            ismutual = np.isin(ids[ids[i]], i).any(axis=1)\n",
    "            ismutual[0] = False\n",
    "            if ismutual.any():\n",
    "                vec_ids.append(i * np.ones(ismutual.sum(), dtype=int))\n",
    "                mut_ids.append(ids[i, ismutual])\n",
    "                mut_sims.append(sims[i, ismutual])\n",
    "        vec_ids, mut_ids, mut_sims = map(np.concatenate, [vec_ids, mut_ids, mut_sims])\n",
    "        affinity = sparse.csc_matrix((mut_sims, (vec_ids, mut_ids)),\n",
    "                                     shape=(num, num), dtype=np.float32)\n",
    "        return affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataset import Dataset\n",
    "from knn import KNN\n",
    "from diffusion import Diffusion\n",
    "from sklearn import preprocessing\n",
    "from evaluate import compute_map_and_print\n",
    "\n",
    "\n",
    "def search():\n",
    "    n_query = len(queries)\n",
    "    diffusion = Diffusion(np.vstack([queries, gallery]), args.cache_dir)\n",
    "    offline = diffusion.get_offline_results(args.truncation_size, args.kd)\n",
    "    features = preprocessing.normalize(offline, norm=\"l2\", axis=1)\n",
    "    scores = features[:n_query] @ features[n_query:].T\n",
    "    ranks = np.argsort(-scores.todense())\n",
    "    evaluate(ranks)\n",
    "\n",
    "\n",
    "def evaluate(ranks):\n",
    "    gnd_name = os.path.splitext(os.path.basename(args.gnd_path))[0]\n",
    "    with open(args.gnd_path, 'rb') as f:\n",
    "        gnd = pickle.load(f)['gnd']\n",
    "    compute_map_and_print(gnd_name.split(\"_\")[-1], ranks.T, gnd)\n",
    "\n",
    "\n",
    "queries, gallery = dataset.queries, dataset.gallery\n",
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T13:30:06.666914Z",
     "start_time": "2020-09-19T13:30:06.648442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DF\n",
    "df_val = pd.read_csv(hparams.config_path / \"train_val.csv\")\n",
    "val_embeddings = torch.tensor(list(map(eval, df_val[\"embeddings\"].values)))\n",
    "query_mask = df_val[\"is_query\"].values.astype(np.bool)\n",
    "val_labels = df_val[\"label\"].values\n",
    "\n",
    "# Shape (n_embeddings, embedding_dim)\n",
    "query_embeddings, gallery_embeddings = val_embeddings[query_mask], val_embeddings[~query_mask]\n",
    "query_labels, gallery_labels = val_labels[query_mask], val_labels[~query_mask]\n",
    "logger.info(f\"Validation query size - {len(query_embeddings)}, gallery size - {len(gallery_embeddings)}\")\n",
    "del val_embeddings\n",
    "\n",
    "if hparams.dba:\n",
    "    gallery_embeddings = query_expansion(gallery_embeddings, gallery_embeddings, topk=10, alpha=None)\n",
    "\n",
    "if hparams.aqe:\n",
    "    query_embeddings = query_expansion(query_embeddings, gallery_embeddings, topk=3, alpha=3)\n",
    "\n",
    "# Shape (query_size x gallery_size)\n",
    "conformity_matrix = torch.tensor(query_labels.reshape(-1, 1) == gallery_labels)\n",
    "\n",
    "# Matrix of pairwise cosin distances\n",
    "distances = torch.cdist(query_embeddings, gallery_embeddings)\n",
    "\n",
    "acc1 = cmc_score_count(distances, conformity_matrix, topk=1)\n",
    "map10 = map_at_k(distances, conformity_matrix, topk=10)\n",
    "mapR = map_at_k(distances, conformity_matrix, topk=None)\n",
    "\n",
    "logger.info(\n",
    "    f\"Val: Acc@1 {acc1:0.5f}, mAP@10 {map10:0.5f}, Target {0.5 * acc1 + 0.5 * map10:0.5f}, mAP@R {mapR:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T13:25:39.600172Z",
     "start_time": "2020-09-19T13:25:39.567783Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "def show_predictions(query_idx, config_path, dba=False, aqe=False, diffusion=False, size=384):\n",
    "    \"\"\"Plots predictions from submission.csv\"\"\"\n",
    "    filename =  f\"submission{'_dba' if dba else ''}{'_aqe' if aqe else ''}_{size}.csv\"\n",
    "    df = pd.read_csv(\n",
    "        pathlib.Path(config_path) / filename, header=None)\n",
    "    df[1] = df[1].apply(lambda x: x[1:])\n",
    "    df[10] = df[10].apply(lambda x: x[:-1])\n",
    "    # Sort, so that results indexes are always same\n",
    "    df.sort_values(by=0, inplace=True)\n",
    "    query_file = os.path.join(\"data/interim/test_data_A_384/query\", df.iloc[query_idx].values[0])\n",
    "    gallery_files = [os.path.join(\"data/interim/test_data_A_384/gallery\", path) for path in df.iloc[query_idx].values[1:]]\n",
    "#     print(query_file, \"\\n\", gallery_files)\n",
    "    query = cv2.imread(query_file)\n",
    "    query = cv2.cvtColor(query, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.subplot(1, 11, 1)\n",
    "    plt.imshow(query)\n",
    "    for i in range(10):\n",
    "        image = cv2.imread(gallery_files[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(1, 11, i + 1)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T13:25:47.442502Z",
     "start_time": "2020-09-19T13:25:47.373773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d43a810d744e84971caf6b6ecae3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4799, description='query_idx', max=9599), Checkbox(value=False, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_predictions(query_idx, config_path, dba=False, aqe=False, diffusion=False, size=384)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_PATH = pathlib.Path(\"logs/genet_small_384_light_arcface80_1/\") \n",
    "SIZE = 384\n",
    "\n",
    "interact_manual(show_predictions, config_path=fixed(CONFIG_PATH), query_idx=(0,9599),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit9890b48dadda4b3ab35703e845f6f232"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
