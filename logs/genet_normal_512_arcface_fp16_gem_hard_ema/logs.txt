[09-16 19:28] - Parameters used for training: Namespace(arch='genet_normal', augmentation='hard', batch_size=96, config_file='configs/genet_normal.yaml', criterion='arcface', criterion_params={'out_features': 3097, 's': 64.0, 'm': 0.5}, debug=False, ema_decay=0.999, embedding_size=512, model_params={}, name='genet_normal_512_arcface_fp16_gem_hard_ema', optim='adamw', outdir='logs/genet_normal_512_arcface_fp16_gem_hard_ema', phases=[{'ep': [0, 50], 'lr': [0.5, 1e-05]}], pooling='gem', resume='', root='data/interim', seed=42, size=512, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=6)
[09-16 19:28] - Start training
[09-16 19:28] - Model size: 19.89M
[09-16 19:28] - Loss for this run is: AdditiveAngularMarginLoss(
  (criterion): CrossEntropyLoss()
)
[09-16 19:28] - Val size: 20643
[09-16 19:28] - Train size: 68811
[09-16 19:28] - Start phase #1 from epoch 0 to epoch 50: {'ep': [0, 50], 'lr': [0.5, 1e-05], 'mom': [], 'mode': 'linear'}
[09-16 19:28] - Epoch 1 | lr 0.00e+00
[09-16 19:33] - 
TimeMeter profiling. Data time: 1.28E-03s. Model time: 4.04E-01s 

[09-16 19:35] - Train loss: 39.9080 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:35] - Val   loss: 42.5368 | Acc_@1: 0.0247 | mAP@10: 0.0394 | .5Acc+.5mAP: 0.0321 | CMC@10: 0.0919
[09-16 19:35] - Epoch  1: best loss improved from inf to 42.5368
[09-16 19:35] - Epoch 2 | lr 4.91e-01
[09-16 19:43] - Train loss: 39.5577 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:43] - Val   loss: 41.2456 | Acc_@1: 0.0029 | mAP@10: 0.0075 | .5Acc+.5mAP: 0.0052 | CMC@10: 0.0247
[09-16 19:43] - Epoch  2: best loss improved from 42.5368 to 41.2456
[09-16 19:43] - Epoch 3 | lr 4.81e-01
[09-16 19:50] - Train loss: 39.3733 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:50] - Val   loss: 39.7488 | Acc_@1: 0.0142 | mAP@10: 0.0275 | .5Acc+.5mAP: 0.0208 | CMC@10: 0.0723
[09-16 19:50] - Epoch  3: best loss improved from 41.2456 to 39.7488
[09-16 19:50] - Epoch 4 | lr 4.71e-01
[09-16 19:57] - Train loss: 39.1837 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:57] - Val   loss: 39.4940 | Acc_@1: 0.0298 | mAP@10: 0.0478 | .5Acc+.5mAP: 0.0388 | CMC@10: 0.1107
[09-16 19:57] - Epoch  4: best loss improved from 39.7488 to 39.4940
[09-16 19:57] - Epoch 5 | lr 4.61e-01
[09-16 20:05] - Train loss: 39.0067 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:05] - Val   loss: 39.3900 | Acc_@1: 0.0487 | mAP@10: 0.0735 | .5Acc+.5mAP: 0.0611 | CMC@10: 0.1606
[09-16 20:05] - Epoch  5: best loss improved from 39.4940 to 39.3900
[09-16 20:05] - Epoch 6 | lr 4.51e-01
[09-16 20:12] - Train loss: 38.8241 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:12] - Val   loss: 39.2397 | Acc_@1: 0.0682 | mAP@10: 0.0971 | .5Acc+.5mAP: 0.0826 | CMC@10: 0.2105
[09-16 20:12] - Epoch  6: best loss improved from 39.3900 to 39.2397
[09-16 20:12] - Epoch 7 | lr 4.41e-01
[09-16 20:20] - Train loss: 38.6158 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:20] - Val   loss: 39.0500 | Acc_@1: 0.0891 | mAP@10: 0.1223 | .5Acc+.5mAP: 0.1057 | CMC@10: 0.2520
[09-16 20:20] - Epoch  7: best loss improved from 39.2397 to 39.0500
[09-16 20:20] - Epoch 8 | lr 4.31e-01
[09-16 20:27] - Train loss: 38.3235 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:27] - Val   loss: 38.8723 | Acc_@1: 0.1191 | mAP@10: 0.1569 | .5Acc+.5mAP: 0.1380 | CMC@10: 0.3166
[09-16 20:27] - Epoch  8: best loss improved from 39.0500 to 38.8723
[09-16 20:27] - Epoch 9 | lr 4.21e-01
[09-16 20:35] - Train loss: 38.0043 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:35] - Val   loss: 38.6363 | Acc_@1: 0.1630 | mAP@10: 0.2025 | .5Acc+.5mAP: 0.1827 | CMC@10: 0.3817
[09-16 20:35] - Epoch  9: best loss improved from 38.8723 to 38.6363
[09-16 20:35] - Epoch 10 | lr 4.11e-01
[09-16 20:42] - Train loss: 37.6579 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:42] - Val   loss: 38.3683 | Acc_@1: 0.2132 | mAP@10: 0.2533 | .5Acc+.5mAP: 0.2332 | CMC@10: 0.4494
[09-16 20:42] - Epoch 10: best loss improved from 38.6363 to 38.3683
[09-16 20:42] - Epoch 11 | lr 4.01e-01
[09-16 20:50] - Train loss: 37.2696 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:50] - Val   loss: 38.0863 | Acc_@1: 0.2669 | mAP@10: 0.3063 | .5Acc+.5mAP: 0.2866 | CMC@10: 0.5137
[09-16 20:50] - Epoch 11: best loss improved from 38.3683 to 38.0863
[09-16 20:50] - Epoch 12 | lr 3.91e-01
[09-16 20:57] - Train loss: 36.8121 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:57] - Val   loss: 37.7465 | Acc_@1: 0.3159 | mAP@10: 0.3536 | .5Acc+.5mAP: 0.3348 | CMC@10: 0.5672
[09-16 20:57] - Epoch 12: best loss improved from 38.0863 to 37.7465
[09-16 20:57] - Epoch 13 | lr 3.81e-01
[09-16 21:05] - Train loss: 36.2858 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:05] - Val   loss: 37.3472 | Acc_@1: 0.3747 | mAP@10: 0.4054 | .5Acc+.5mAP: 0.3901 | CMC@10: 0.6253
[09-16 21:05] - Epoch 13: best loss improved from 37.7465 to 37.3472
[09-16 21:05] - Epoch 14 | lr 3.71e-01
[09-16 21:12] - Train loss: 35.6979 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:12] - Val   loss: 36.9115 | Acc_@1: 0.4275 | mAP@10: 0.4519 | .5Acc+.5mAP: 0.4397 | CMC@10: 0.6743
[09-16 21:12] - Epoch 14: best loss improved from 37.3472 to 36.9115
[09-16 21:12] - Epoch 15 | lr 3.61e-01
[09-16 21:20] - Train loss: 35.0672 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:20] - Val   loss: 36.4028 | Acc_@1: 0.4690 | mAP@10: 0.4910 | .5Acc+.5mAP: 0.4800 | CMC@10: 0.7062
[09-16 21:20] - Epoch 15: best loss improved from 36.9115 to 36.4028
[09-16 21:20] - Epoch 16 | lr 3.51e-01
[09-16 21:27] - Train loss: 34.3965 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:27] - Val   loss: 35.8623 | Acc_@1: 0.5103 | mAP@10: 0.5315 | .5Acc+.5mAP: 0.5209 | CMC@10: 0.7441
[09-16 21:27] - Epoch 16: best loss improved from 36.4028 to 35.8623
[09-16 21:27] - Epoch 17 | lr 3.41e-01
[09-16 21:35] - Train loss: 33.6502 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:35] - Val   loss: 35.2659 | Acc_@1: 0.5521 | mAP@10: 0.5687 | .5Acc+.5mAP: 0.5604 | CMC@10: 0.7768
[09-16 21:35] - Epoch 17: best loss improved from 35.8623 to 35.2659
[09-16 21:35] - Epoch 18 | lr 3.31e-01
[09-16 21:42] - Train loss: 32.9573 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:42] - Val   loss: 34.7134 | Acc_@1: 0.5871 | mAP@10: 0.5992 | .5Acc+.5mAP: 0.5931 | CMC@10: 0.7993
[09-16 21:42] - Epoch 18: best loss improved from 35.2659 to 34.7134
[09-16 21:42] - Epoch 19 | lr 3.21e-01
[09-16 21:50] - Train loss: 32.1805 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:50] - Val   loss: 34.2204 | Acc_@1: 0.6128 | mAP@10: 0.6218 | .5Acc+.5mAP: 0.6173 | CMC@10: 0.8169
[09-16 21:50] - Epoch 19: best loss improved from 34.7134 to 34.2204
[09-16 21:50] - Epoch 20 | lr 3.11e-01
[09-16 21:57] - Train loss: 31.3442 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:57] - Val   loss: 33.6942 | Acc_@1: 0.6387 | mAP@10: 0.6445 | .5Acc+.5mAP: 0.6416 | CMC@10: 0.8329
[09-16 21:57] - Epoch 20: best loss improved from 34.2204 to 33.6942
[09-16 21:57] - Epoch 21 | lr 3.01e-01
[09-16 22:05] - Train loss: 30.5314 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:05] - Val   loss: 33.1760 | Acc_@1: 0.6510 | mAP@10: 0.6634 | .5Acc+.5mAP: 0.6572 | CMC@10: 0.8469
[09-16 22:05] - Epoch 21: best loss improved from 33.6942 to 33.1760
[09-16 22:05] - Epoch 22 | lr 2.91e-01
[09-16 22:12] - Train loss: 29.6577 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:12] - Val   loss: 32.6518 | Acc_@1: 0.6692 | mAP@10: 0.6774 | .5Acc+.5mAP: 0.6733 | CMC@10: 0.8560
[09-16 22:12] - Epoch 22: best loss improved from 33.1760 to 32.6518
[09-16 22:12] - Epoch 23 | lr 2.81e-01
[09-16 22:20] - Train loss: 28.8318 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:20] - Val   loss: 32.1034 | Acc_@1: 0.6839 | mAP@10: 0.6903 | .5Acc+.5mAP: 0.6871 | CMC@10: 0.8646
[09-16 22:20] - Epoch 23: best loss improved from 32.6518 to 32.1034
[09-16 22:20] - Epoch 24 | lr 2.71e-01
[09-16 22:27] - Train loss: 27.9550 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:27] - Val   loss: 31.6495 | Acc_@1: 0.7036 | mAP@10: 0.7058 | .5Acc+.5mAP: 0.7047 | CMC@10: 0.8716
[09-16 22:27] - Epoch 24: best loss improved from 32.1034 to 31.6495
[09-16 22:27] - Epoch 25 | lr 2.61e-01
[09-16 22:35] - Train loss: 27.1112 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:35] - Val   loss: 31.1602 | Acc_@1: 0.7136 | mAP@10: 0.7161 | .5Acc+.5mAP: 0.7149 | CMC@10: 0.8776
[09-16 22:35] - Epoch 25: best loss improved from 31.6495 to 31.1602
[09-16 22:35] - Epoch 26 | lr 2.51e-01
[09-16 22:42] - Train loss: 26.2471 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:42] - Val   loss: 30.7434 | Acc_@1: 0.7204 | mAP@10: 0.7220 | .5Acc+.5mAP: 0.7212 | CMC@10: 0.8802
[09-16 22:42] - Epoch 26: best loss improved from 31.1602 to 30.7434
[09-16 22:42] - Epoch 27 | lr 2.41e-01
[09-16 22:50] - Train loss: 25.3584 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:50] - Val   loss: 30.3228 | Acc_@1: 0.7300 | mAP@10: 0.7287 | .5Acc+.5mAP: 0.7293 | CMC@10: 0.8805
[09-16 22:50] - Epoch 27: best loss improved from 30.7434 to 30.3228
[09-16 22:50] - Epoch 28 | lr 2.31e-01
[09-16 22:57] - Train loss: 24.4559 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:57] - Val   loss: 29.7927 | Acc_@1: 0.7340 | mAP@10: 0.7378 | .5Acc+.5mAP: 0.7359 | CMC@10: 0.8838
[09-16 22:57] - Epoch 28: best loss improved from 30.3228 to 29.7927
[09-16 22:57] - Epoch 29 | lr 2.21e-01
[09-16 23:05] - Train loss: 23.6599 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:05] - Val   loss: 29.3785 | Acc_@1: 0.7446 | mAP@10: 0.7443 | .5Acc+.5mAP: 0.7445 | CMC@10: 0.8877
[09-16 23:05] - Epoch 29: best loss improved from 29.7927 to 29.3785
[09-16 23:05] - Epoch 30 | lr 2.11e-01
[09-16 23:12] - Train loss: 22.8118 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:12] - Val   loss: 29.0003 | Acc_@1: 0.7489 | mAP@10: 0.7478 | .5Acc+.5mAP: 0.7483 | CMC@10: 0.8862
[09-16 23:12] - Epoch 30: best loss improved from 29.3785 to 29.0003
[09-16 23:12] - Epoch 31 | lr 2.01e-01
[09-16 23:20] - Train loss: 21.9865 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:20] - Val   loss: 28.6143 | Acc_@1: 0.7499 | mAP@10: 0.7530 | .5Acc+.5mAP: 0.7515 | CMC@10: 0.8881
[09-16 23:20] - Epoch 31: best loss improved from 29.0003 to 28.6143
[09-16 23:20] - Epoch 32 | lr 1.91e-01
[09-16 23:27] - Train loss: 21.2177 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:27] - Val   loss: 28.3375 | Acc_@1: 0.7561 | mAP@10: 0.7556 | .5Acc+.5mAP: 0.7559 | CMC@10: 0.8893
[09-16 23:27] - Epoch 32: best loss improved from 28.6143 to 28.3375
[09-16 23:27] - Epoch 33 | lr 1.81e-01
[09-16 23:35] - Train loss: 20.3674 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:35] - Val   loss: 28.0810 | Acc_@1: 0.7590 | mAP@10: 0.7565 | .5Acc+.5mAP: 0.7578 | CMC@10: 0.8838
[09-16 23:35] - Epoch 33: best loss improved from 28.3375 to 28.0810
[09-16 23:35] - Epoch 34 | lr 1.71e-01
[09-16 23:42] - Train loss: 19.5989 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:42] - Val   loss: 27.9676 | Acc_@1: 0.7607 | mAP@10: 0.7597 | .5Acc+.5mAP: 0.7602 | CMC@10: 0.8829
[09-16 23:42] - Epoch 34: best loss improved from 28.0810 to 27.9676
[09-16 23:42] - Epoch 35 | lr 1.61e-01
[09-16 23:50] - Train loss: 18.8683 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:50] - Val   loss: 27.6202 | Acc_@1: 0.7631 | mAP@10: 0.7619 | .5Acc+.5mAP: 0.7625 | CMC@10: 0.8850
[09-16 23:50] - Epoch 35: best loss improved from 27.9676 to 27.6202
[09-16 23:50] - Epoch 36 | lr 1.51e-01
[09-16 23:57] - Train loss: 18.1282 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 23:57] - Val   loss: 27.5792 | Acc_@1: 0.7595 | mAP@10: 0.7568 | .5Acc+.5mAP: 0.7581 | CMC@10: 0.8773
[09-16 23:57] - Epoch 36: best loss improved from 27.6202 to 27.5792
[09-16 23:57] - Epoch 37 | lr 1.41e-01
[09-17 00:05] - Train loss: 17.4224 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:05] - Val   loss: 27.5152 | Acc_@1: 0.7643 | mAP@10: 0.7602 | .5Acc+.5mAP: 0.7623 | CMC@10: 0.8759
[09-17 00:05] - Epoch 37: best loss improved from 27.5792 to 27.5152
[09-17 00:05] - Epoch 38 | lr 1.31e-01
[09-17 00:12] - Train loss: 16.7609 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:12] - Val   loss: 27.5186 | Acc_@1: 0.7621 | mAP@10: 0.7597 | .5Acc+.5mAP: 0.7609 | CMC@10: 0.8745
[09-17 00:12] - Epoch 39 | lr 1.21e-01
[09-17 00:20] - Train loss: 16.1056 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:20] - Val   loss: 27.4544 | Acc_@1: 0.7638 | mAP@10: 0.7614 | .5Acc+.5mAP: 0.7626 | CMC@10: 0.8725
[09-17 00:20] - Epoch 39: best loss improved from 27.5152 to 27.4544
[09-17 00:20] - Epoch 40 | lr 1.11e-01
[09-17 00:28] - Train loss: 15.5335 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:28] - Val   loss: 27.5324 | Acc_@1: 0.7607 | mAP@10: 0.7583 | .5Acc+.5mAP: 0.7595 | CMC@10: 0.8704
[09-17 00:28] - Epoch 41 | lr 1.01e-01
[09-17 00:35] - Train loss: 14.9168 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:35] - Val   loss: 27.4229 | Acc_@1: 0.7607 | mAP@10: 0.7587 | .5Acc+.5mAP: 0.7597 | CMC@10: 0.8694
[09-17 00:35] - Epoch 41: best loss improved from 27.4544 to 27.4229
[09-17 00:35] - Epoch 42 | lr 9.07e-02
[09-17 00:43] - Train loss: 14.3873 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:43] - Val   loss: 27.5270 | Acc_@1: 0.7607 | mAP@10: 0.7572 | .5Acc+.5mAP: 0.7589 | CMC@10: 0.8699
[09-17 00:43] - Epoch 43 | lr 8.07e-02
[09-17 00:50] - Train loss: 13.8304 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:50] - Val   loss: 27.7356 | Acc_@1: 0.7585 | mAP@10: 0.7564 | .5Acc+.5mAP: 0.7575 | CMC@10: 0.8658
[09-17 00:50] - Epoch 44 | lr 7.07e-02
[09-17 00:58] - Train loss: 13.3402 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 00:58] - Val   loss: 27.8684 | Acc_@1: 0.7597 | mAP@10: 0.7567 | .5Acc+.5mAP: 0.7582 | CMC@10: 0.8632
[09-17 00:58] - Epoch 45 | lr 6.07e-02
[09-17 01:06] - Train loss: 12.9109 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:06] - Val   loss: 28.1253 | Acc_@1: 0.7518 | mAP@10: 0.7528 | .5Acc+.5mAP: 0.7523 | CMC@10: 0.8567
[09-17 01:06] - Epoch 46 | lr 5.07e-02
[09-17 01:13] - Train loss: 12.4919 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:13] - Val   loss: 28.3424 | Acc_@1: 0.7535 | mAP@10: 0.7516 | .5Acc+.5mAP: 0.7525 | CMC@10: 0.8541
[09-17 01:13] - Epoch 47 | lr 4.07e-02
[09-17 01:21] - Train loss: 12.1409 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:21] - Val   loss: 28.5262 | Acc_@1: 0.7470 | mAP@10: 0.7460 | .5Acc+.5mAP: 0.7465 | CMC@10: 0.8526
[09-17 01:21] - Epoch 48 | lr 3.08e-02
[09-17 01:28] - Train loss: 11.8093 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:28] - Val   loss: 28.7929 | Acc_@1: 0.7484 | mAP@10: 0.7454 | .5Acc+.5mAP: 0.7469 | CMC@10: 0.8502
[09-17 01:28] - Epoch 49 | lr 2.08e-02
[09-17 01:36] - Train loss: 11.5433 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:36] - Val   loss: 29.0485 | Acc_@1: 0.7439 | mAP@10: 0.7427 | .5Acc+.5mAP: 0.7433 | CMC@10: 0.8459
[09-17 01:36] - Epoch 50 | lr 1.08e-02
[09-17 01:43] - Train loss: 11.3870 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-17 01:43] - Val   loss: 29.3105 | Acc_@1: 0.7434 | mAP@10: 0.7402 | .5Acc+.5mAP: 0.7418 | CMC@10: 0.8435
[09-17 01:43] - Loading best model from previous phase
[09-17 01:43] - Finished Training. Took: 375.57m
