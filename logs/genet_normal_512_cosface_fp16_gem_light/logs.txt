[09-16 15:40] - Parameters used for training: Namespace(arch='genet_normal', augmentation='light', batch_size=96, config_file='configs/genet_normal.yaml', criterion='cosface', criterion_params={'out_features': 3097, 's': 30.0, 'm': 0.5}, debug=False, ema_decay=0.0, embedding_size=512, model_params={}, name='genet_normal_512_cosface_fp16_gem_light', optim='adamw', outdir='logs/genet_normal_512_cosface_fp16_gem_light', phases=[{'ep': [0, 50], 'lr': [0.05, 1e-05]}], pooling='gem', resume='', root='data/interim', seed=42, size=512, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=6)
[09-16 15:40] - Start training
[09-16 15:40] - Model size: 19.89M
[09-16 15:40] - Loss for this run is: LargeMarginCosineLoss(
  (criterion): CrossEntropyLoss()
)
[09-16 15:40] - Val size: 20641
[09-16 15:40] - Train size: 68805
[09-16 15:40] - Start phase #1 from epoch 0 to epoch 50: {'ep': [0, 50], 'lr': [0.05, 1e-05], 'mom': [], 'mode': 'linear'}
[09-16 15:40] - Epoch 1 | lr 0.00e+00
[09-16 15:46] - 
TimeMeter profiling. Data time: 6.95E-04s. Model time: 4.22E-01s 

[09-16 15:48] - Train loss: 20.5723 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 15:48] - Val   loss: 17.9661 | Acc_@1: 0.7812 | mAP@10: 0.7745 | .5Acc+.5mAP: 0.7778 | CMC@10: 0.9092
[09-16 15:48] - Epoch  1: best loss improved from inf to 17.9661
[09-16 15:48] - Epoch 2 | lr 4.91e-02
[09-16 15:55] - Train loss: 15.2254 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 15:55] - Val   loss: 14.4441 | Acc_@1: 0.8544 | mAP@10: 0.8487 | .5Acc+.5mAP: 0.8516 | CMC@10: 0.9529
[09-16 15:55] - Epoch  2: best loss improved from 17.9661 to 14.4441
[09-16 15:55] - Epoch 3 | lr 4.81e-02
[09-16 16:03] - Train loss: 11.8504 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:03] - Val   loss: 12.5799 | Acc_@1: 0.8681 | mAP@10: 0.8667 | .5Acc+.5mAP: 0.8674 | CMC@10: 0.9556
[09-16 16:03] - Epoch  3: best loss improved from 14.4441 to 12.5799
[09-16 16:03] - Epoch 4 | lr 4.71e-02
[09-16 16:10] - Train loss: 9.6411 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:10] - Val   loss: 10.6507 | Acc_@1: 0.9082 | mAP@10: 0.9005 | .5Acc+.5mAP: 0.9044 | CMC@10: 0.9702
[09-16 16:10] - Epoch  4: best loss improved from 12.5799 to 10.6507
[09-16 16:10] - Epoch 5 | lr 4.61e-02
[09-16 16:18] - Train loss: 8.1060 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:18] - Val   loss: 9.3235 | Acc_@1: 0.9063 | mAP@10: 0.9030 | .5Acc+.5mAP: 0.9047 | CMC@10: 0.9652
[09-16 16:18] - Epoch  5: best loss improved from 10.6507 to 9.3235
[09-16 16:18] - Epoch 6 | lr 4.51e-02
[09-16 16:25] - Train loss: 6.9776 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:25] - Val   loss: 8.4957 | Acc_@1: 0.9142 | mAP@10: 0.9146 | .5Acc+.5mAP: 0.9144 | CMC@10: 0.9724
[09-16 16:25] - Epoch  6: best loss improved from 9.3235 to 8.4957
[09-16 16:25] - Epoch 7 | lr 4.41e-02
[09-16 16:32] - Train loss: 6.0661 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:32] - Val   loss: 7.6776 | Acc_@1: 0.9308 | mAP@10: 0.9290 | .5Acc+.5mAP: 0.9299 | CMC@10: 0.9784
[09-16 16:32] - Epoch  7: best loss improved from 8.4957 to 7.6776
[09-16 16:32] - Epoch 8 | lr 4.31e-02
[09-16 16:40] - Train loss: 5.3556 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:40] - Val   loss: 7.1319 | Acc_@1: 0.9342 | mAP@10: 0.9353 | .5Acc+.5mAP: 0.9347 | CMC@10: 0.9817
[09-16 16:40] - Epoch  8: best loss improved from 7.6776 to 7.1319
[09-16 16:40] - Epoch 9 | lr 4.21e-02
[09-16 16:47] - Train loss: 4.8275 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:47] - Val   loss: 6.7766 | Acc_@1: 0.9363 | mAP@10: 0.9369 | .5Acc+.5mAP: 0.9366 | CMC@10: 0.9786
[09-16 16:47] - Epoch  9: best loss improved from 7.1319 to 6.7766
[09-16 16:47] - Epoch 10 | lr 4.11e-02
[09-16 16:55] - Train loss: 4.3835 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 16:55] - Val   loss: 6.4787 | Acc_@1: 0.9308 | mAP@10: 0.9338 | .5Acc+.5mAP: 0.9323 | CMC@10: 0.9786
[09-16 16:55] - Epoch 10: best loss improved from 6.7766 to 6.4787
[09-16 16:55] - Epoch 11 | lr 4.01e-02
[09-16 17:02] - Train loss: 3.9747 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:02] - Val   loss: 6.0583 | Acc_@1: 0.9414 | mAP@10: 0.9410 | .5Acc+.5mAP: 0.9412 | CMC@10: 0.9791
[09-16 17:02] - Epoch 11: best loss improved from 6.4787 to 6.0583
[09-16 17:02] - Epoch 12 | lr 3.91e-02
[09-16 17:10] - Train loss: 3.6389 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:10] - Val   loss: 6.0552 | Acc_@1: 0.9498 | mAP@10: 0.9456 | .5Acc+.5mAP: 0.9477 | CMC@10: 0.9815
[09-16 17:10] - Epoch 12: best loss improved from 6.0583 to 6.0552
[09-16 17:10] - Epoch 13 | lr 3.81e-02
[09-16 17:17] - Train loss: 3.3748 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:17] - Val   loss: 5.4689 | Acc_@1: 0.9522 | mAP@10: 0.9526 | .5Acc+.5mAP: 0.9524 | CMC@10: 0.9861
[09-16 17:17] - Epoch 13: best loss improved from 6.0552 to 5.4689
[09-16 17:17] - Epoch 14 | lr 3.71e-02
[09-16 17:25] - Train loss: 3.0889 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:25] - Val   loss: 5.7728 | Acc_@1: 0.9452 | mAP@10: 0.9456 | .5Acc+.5mAP: 0.9454 | CMC@10: 0.9815
[09-16 17:25] - Epoch 15 | lr 3.61e-02
[09-16 17:33] - Train loss: 2.8373 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:33] - Val   loss: 5.3754 | Acc_@1: 0.9553 | mAP@10: 0.9555 | .5Acc+.5mAP: 0.9554 | CMC@10: 0.9868
[09-16 17:33] - Epoch 15: best loss improved from 5.4689 to 5.3754
[09-16 17:33] - Epoch 16 | lr 3.51e-02
[09-16 17:41] - Train loss: 2.6864 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:41] - Val   loss: 4.9793 | Acc_@1: 0.9556 | mAP@10: 0.9569 | .5Acc+.5mAP: 0.9562 | CMC@10: 0.9877
[09-16 17:41] - Epoch 16: best loss improved from 5.3754 to 4.9793
[09-16 17:41] - Epoch 17 | lr 3.41e-02
[09-16 17:48] - Train loss: 2.4660 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:48] - Val   loss: 4.9048 | Acc_@1: 0.9556 | mAP@10: 0.9567 | .5Acc+.5mAP: 0.9561 | CMC@10: 0.9846
[09-16 17:48] - Epoch 17: best loss improved from 4.9793 to 4.9048
[09-16 17:48] - Epoch 18 | lr 3.31e-02
[09-16 17:56] - Train loss: 2.3318 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 17:56] - Val   loss: 4.9006 | Acc_@1: 0.9608 | mAP@10: 0.9619 | .5Acc+.5mAP: 0.9614 | CMC@10: 0.9906
[09-16 17:56] - Epoch 18: best loss improved from 4.9048 to 4.9006
[09-16 17:56] - Epoch 19 | lr 3.21e-02
[09-16 18:04] - Train loss: 2.1610 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:04] - Val   loss: 4.5379 | Acc_@1: 0.9616 | mAP@10: 0.9627 | .5Acc+.5mAP: 0.9621 | CMC@10: 0.9890
[09-16 18:04] - Epoch 19: best loss improved from 4.9006 to 4.5379
[09-16 18:04] - Epoch 20 | lr 3.11e-02
[09-16 18:12] - Train loss: 2.0252 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:12] - Val   loss: 4.5908 | Acc_@1: 0.9608 | mAP@10: 0.9621 | .5Acc+.5mAP: 0.9615 | CMC@10: 0.9877
[09-16 18:12] - Epoch 21 | lr 3.01e-02
[09-16 18:19] - Train loss: 1.9108 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:19] - Val   loss: 4.3089 | Acc_@1: 0.9683 | mAP@10: 0.9686 | .5Acc+.5mAP: 0.9684 | CMC@10: 0.9914
[09-16 18:20] - Epoch 21: best loss improved from 4.5379 to 4.3089
[09-16 18:20] - Epoch 22 | lr 2.91e-02
[09-16 18:27] - Train loss: 1.7796 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:27] - Val   loss: 4.2863 | Acc_@1: 0.9673 | mAP@10: 0.9673 | .5Acc+.5mAP: 0.9673 | CMC@10: 0.9911
[09-16 18:27] - Epoch 22: best loss improved from 4.3089 to 4.2863
[09-16 18:27] - Epoch 23 | lr 2.81e-02
[09-16 18:35] - Train loss: 1.6912 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:35] - Val   loss: 4.0456 | Acc_@1: 0.9669 | mAP@10: 0.9683 | .5Acc+.5mAP: 0.9676 | CMC@10: 0.9911
[09-16 18:35] - Epoch 23: best loss improved from 4.2863 to 4.0456
[09-16 18:35] - Epoch 24 | lr 2.71e-02
[09-16 18:43] - Train loss: 1.6119 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:43] - Val   loss: 4.2073 | Acc_@1: 0.9700 | mAP@10: 0.9713 | .5Acc+.5mAP: 0.9706 | CMC@10: 0.9914
[09-16 18:43] - Epoch 25 | lr 2.61e-02
[09-16 18:51] - Train loss: 1.5160 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:51] - Val   loss: 4.0789 | Acc_@1: 0.9666 | mAP@10: 0.9677 | .5Acc+.5mAP: 0.9672 | CMC@10: 0.9906
[09-16 18:51] - Epoch 26 | lr 2.51e-02
[09-16 18:59] - Train loss: 1.4279 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 18:59] - Val   loss: 4.1469 | Acc_@1: 0.9693 | mAP@10: 0.9700 | .5Acc+.5mAP: 0.9696 | CMC@10: 0.9906
[09-16 18:59] - Epoch 27 | lr 2.41e-02
[09-16 19:07] - Train loss: 1.3436 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:07] - Val   loss: 3.7845 | Acc_@1: 0.9712 | mAP@10: 0.9727 | .5Acc+.5mAP: 0.9719 | CMC@10: 0.9928
[09-16 19:07] - Epoch 27: best loss improved from 4.0456 to 3.7845
[09-16 19:07] - Epoch 28 | lr 2.31e-02
[09-16 19:14] - Train loss: 1.2889 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:14] - Val   loss: 3.8075 | Acc_@1: 0.9750 | mAP@10: 0.9734 | .5Acc+.5mAP: 0.9742 | CMC@10: 0.9899
[09-16 19:14] - Epoch 29 | lr 2.21e-02
[09-16 19:22] - Train loss: 1.1799 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:22] - Val   loss: 3.6124 | Acc_@1: 0.9721 | mAP@10: 0.9740 | .5Acc+.5mAP: 0.9731 | CMC@10: 0.9918
[09-16 19:22] - Epoch 29: best loss improved from 3.7845 to 3.6124
[09-16 19:22] - Epoch 30 | lr 2.11e-02
[09-16 19:30] - Train loss: 1.1674 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:30] - Val   loss: 3.6185 | Acc_@1: 0.9721 | mAP@10: 0.9743 | .5Acc+.5mAP: 0.9732 | CMC@10: 0.9923
[09-16 19:30] - Epoch 31 | lr 2.01e-02
[09-16 19:37] - Train loss: 1.0885 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:37] - Val   loss: 3.4515 | Acc_@1: 0.9762 | mAP@10: 0.9775 | .5Acc+.5mAP: 0.9769 | CMC@10: 0.9950
[09-16 19:37] - Epoch 31: best loss improved from 3.6124 to 3.4515
[09-16 19:37] - Epoch 32 | lr 1.91e-02
[09-16 19:45] - Train loss: 1.0428 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:45] - Val   loss: 3.4418 | Acc_@1: 0.9781 | mAP@10: 0.9788 | .5Acc+.5mAP: 0.9784 | CMC@10: 0.9954
[09-16 19:45] - Epoch 32: best loss improved from 3.4515 to 3.4418
[09-16 19:45] - Epoch 33 | lr 1.81e-02
[09-16 19:53] - Train loss: 1.0025 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 19:53] - Val   loss: 3.3361 | Acc_@1: 0.9772 | mAP@10: 0.9784 | .5Acc+.5mAP: 0.9778 | CMC@10: 0.9952
[09-16 19:53] - Epoch 33: best loss improved from 3.4418 to 3.3361
[09-16 19:53] - Epoch 34 | lr 1.71e-02
[09-16 20:00] - Train loss: 0.9628 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:00] - Val   loss: 3.2942 | Acc_@1: 0.9784 | mAP@10: 0.9783 | .5Acc+.5mAP: 0.9784 | CMC@10: 0.9950
[09-16 20:00] - Epoch 34: best loss improved from 3.3361 to 3.2942
[09-16 20:00] - Epoch 35 | lr 1.61e-02
[09-16 20:08] - Train loss: 0.9457 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:08] - Val   loss: 3.3424 | Acc_@1: 0.9779 | mAP@10: 0.9793 | .5Acc+.5mAP: 0.9786 | CMC@10: 0.9942
[09-16 20:08] - Epoch 36 | lr 1.51e-02
[09-16 20:15] - Train loss: 0.9008 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:15] - Val   loss: 3.2368 | Acc_@1: 0.9801 | mAP@10: 0.9796 | .5Acc+.5mAP: 0.9798 | CMC@10: 0.9945
[09-16 20:15] - Epoch 36: best loss improved from 3.2942 to 3.2368
[09-16 20:15] - Epoch 37 | lr 1.41e-02
[09-16 20:23] - Train loss: 0.8776 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:23] - Val   loss: 3.1864 | Acc_@1: 0.9815 | mAP@10: 0.9810 | .5Acc+.5mAP: 0.9812 | CMC@10: 0.9954
[09-16 20:23] - Epoch 37: best loss improved from 3.2368 to 3.1864
[09-16 20:23] - Epoch 38 | lr 1.31e-02
[09-16 20:31] - Train loss: 0.8436 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:31] - Val   loss: 3.1643 | Acc_@1: 0.9805 | mAP@10: 0.9795 | .5Acc+.5mAP: 0.9800 | CMC@10: 0.9940
[09-16 20:31] - Epoch 38: best loss improved from 3.1864 to 3.1643
[09-16 20:31] - Epoch 39 | lr 1.21e-02
[09-16 20:38] - Train loss: 0.8391 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:38] - Val   loss: 3.1590 | Acc_@1: 0.9786 | mAP@10: 0.9803 | .5Acc+.5mAP: 0.9794 | CMC@10: 0.9962
[09-16 20:38] - Epoch 39: best loss improved from 3.1643 to 3.1590
[09-16 20:38] - Epoch 40 | lr 1.11e-02
[09-16 20:46] - Train loss: 0.8085 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:46] - Val   loss: 3.1040 | Acc_@1: 0.9815 | mAP@10: 0.9823 | .5Acc+.5mAP: 0.9819 | CMC@10: 0.9969
[09-16 20:46] - Epoch 40: best loss improved from 3.1590 to 3.1040
[09-16 20:46] - Epoch 41 | lr 1.01e-02
[09-16 20:53] - Train loss: 0.7977 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 20:53] - Val   loss: 2.9847 | Acc_@1: 0.9820 | mAP@10: 0.9819 | .5Acc+.5mAP: 0.9820 | CMC@10: 0.9952
[09-16 20:53] - Epoch 41: best loss improved from 3.1040 to 2.9847
[09-16 20:53] - Epoch 42 | lr 9.08e-03
[09-16 21:01] - Train loss: 0.7727 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:01] - Val   loss: 2.9689 | Acc_@1: 0.9817 | mAP@10: 0.9823 | .5Acc+.5mAP: 0.9820 | CMC@10: 0.9957
[09-16 21:01] - Epoch 42: best loss improved from 2.9847 to 2.9689
[09-16 21:01] - Epoch 43 | lr 8.08e-03
[09-16 21:11] - Train loss: 0.7678 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:11] - Val   loss: 2.9435 | Acc_@1: 0.9820 | mAP@10: 0.9826 | .5Acc+.5mAP: 0.9823 | CMC@10: 0.9954
[09-16 21:11] - Epoch 43: best loss improved from 2.9689 to 2.9435
[09-16 21:11] - Epoch 44 | lr 7.08e-03
[09-16 21:19] - Train loss: 0.7586 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:19] - Val   loss: 2.9657 | Acc_@1: 0.9803 | mAP@10: 0.9814 | .5Acc+.5mAP: 0.9809 | CMC@10: 0.9964
[09-16 21:19] - Epoch 45 | lr 6.08e-03
[09-16 21:27] - Train loss: 0.7419 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:27] - Val   loss: 2.9328 | Acc_@1: 0.9815 | mAP@10: 0.9820 | .5Acc+.5mAP: 0.9818 | CMC@10: 0.9959
[09-16 21:27] - Epoch 45: best loss improved from 2.9435 to 2.9328
[09-16 21:27] - Epoch 46 | lr 5.08e-03
[09-16 21:35] - Train loss: 0.7221 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:35] - Val   loss: 2.8976 | Acc_@1: 0.9832 | mAP@10: 0.9831 | .5Acc+.5mAP: 0.9831 | CMC@10: 0.9959
[09-16 21:35] - Epoch 46: best loss improved from 2.9328 to 2.8976
[09-16 21:35] - Epoch 47 | lr 4.08e-03
[09-16 21:43] - Train loss: 0.7272 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:43] - Val   loss: 2.8805 | Acc_@1: 0.9817 | mAP@10: 0.9825 | .5Acc+.5mAP: 0.9821 | CMC@10: 0.9966
[09-16 21:43] - Epoch 47: best loss improved from 2.8976 to 2.8805
[09-16 21:43] - Epoch 48 | lr 3.08e-03
[09-16 21:50] - Train loss: 0.7122 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:50] - Val   loss: 2.8387 | Acc_@1: 0.9829 | mAP@10: 0.9830 | .5Acc+.5mAP: 0.9830 | CMC@10: 0.9959
[09-16 21:50] - Epoch 48: best loss improved from 2.8805 to 2.8387
[09-16 21:50] - Epoch 49 | lr 2.08e-03
[09-16 21:58] - Train loss: 0.7086 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 21:58] - Val   loss: 2.8057 | Acc_@1: 0.9827 | mAP@10: 0.9826 | .5Acc+.5mAP: 0.9826 | CMC@10: 0.9954
[09-16 21:58] - Epoch 49: best loss improved from 2.8387 to 2.8057
[09-16 21:58] - Epoch 50 | lr 1.08e-03
[09-16 22:06] - Train loss: 0.7094 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 22:06] - Val   loss: 2.8188 | Acc_@1: 0.9827 | mAP@10: 0.9826 | .5Acc+.5mAP: 0.9827 | CMC@10: 0.9962
[09-16 22:06] - Loading best model from previous phase
[09-16 22:06] - Finished Training. Took: 385.79m
