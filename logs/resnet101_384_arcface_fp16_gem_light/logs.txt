[09-16 00:10] - Parameters used for training: Namespace(arch='resnet101', augmentation='light', batch_size=80, config_file='configs/resnet101.yaml', criterion='arcface', criterion_params={'out_features': 3097, 's': 64.0, 'm': 0.3}, debug=False, ema_decay=0.0, embedding_size=512, model_params={}, name='resnet101_384_arcface_fp16_gem_light', optim='adamw', outdir='logs/resnet101_384_arcface_fp16_gem_light', phases=[{'ep': [0, 50], 'lr': [0.1, 1e-05]}], pooling='gem', resume='', root='data/interim', seed=42, size=384, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=10)
[09-16 00:10] - Start training
[09-16 00:10] - Model size: 43.55M
[09-16 00:10] - Loss for this run is: AdditiveAngularMarginLoss(
  (criterion): CrossEntropyLoss()
)
[09-16 00:10] - Val size: 13761
[09-16 00:10] - Train size: 68805
[09-16 00:10] - Start phase #1 from epoch 0 to epoch 50: {'ep': [0, 50], 'lr': [0.1, 1e-05], 'mom': [], 'mode': 'linear'}
[09-16 00:10] - Epoch 1 | lr 0.00e+00
[09-16 00:18] - 
TimeMeter profiling. Data time: 1.03E-03s. Model time: 5.10E-01s 

[09-16 00:23] - Train loss: 29.1889 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 00:23] - Val   loss: 28.3984 | Acc_@1: 0.0083 | mAP@10: 0.0152 | .5Acc+.5mAP: 0.0117 | CMC@10: 0.0418
[09-16 00:23] - Epoch  1: best loss improved from inf to 28.3984
[09-16 00:23] - Epoch 2 | lr 9.83e-02
[09-16 00:35] - Train loss: 28.0261 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 00:35] - Val   loss: 27.9425 | Acc_@1: 0.0090 | mAP@10: 0.0179 | .5Acc+.5mAP: 0.0134 | CMC@10: 0.0536
[09-16 00:35] - Epoch  2: best loss improved from 28.3984 to 27.9425
[09-16 00:35] - Epoch 3 | lr 9.63e-02
[09-16 00:45] - Train loss: 27.8635 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 00:45] - Val   loss: 27.8849 | Acc_@1: 0.0209 | mAP@10: 0.0354 | .5Acc+.5mAP: 0.0281 | CMC@10: 0.0886
[09-16 00:45] - Epoch  3: best loss improved from 27.9425 to 27.8849
[09-16 00:45] - Epoch 4 | lr 9.43e-02
[09-16 00:57] - Train loss: 27.7627 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 00:57] - Val   loss: 27.8035 | Acc_@1: 0.0302 | mAP@10: 0.0463 | .5Acc+.5mAP: 0.0383 | CMC@10: 0.1127
[09-16 00:57] - Epoch  4: best loss improved from 27.8849 to 27.8035
[09-16 00:57] - Epoch 5 | lr 9.23e-02
[09-16 01:09] - Train loss: 27.6701 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 01:09] - Val   loss: 27.7170 | Acc_@1: 0.0331 | mAP@10: 0.0577 | .5Acc+.5mAP: 0.0454 | CMC@10: 0.1451
[09-16 01:09] - Epoch  5: best loss improved from 27.8035 to 27.7170
[09-16 01:09] - Epoch 6 | lr 9.03e-02
[09-16 01:23] - Train loss: 27.5625 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 01:23] - Val   loss: 27.5698 | Acc_@1: 0.0486 | mAP@10: 0.0822 | .5Acc+.5mAP: 0.0654 | CMC@10: 0.1962
[09-16 01:23] - Epoch  6: best loss improved from 27.7170 to 27.5698
[09-16 01:23] - Epoch 7 | lr 8.83e-02
[09-16 01:38] - Train loss: 27.4321 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 01:38] - Val   loss: 27.5119 | Acc_@1: 0.0709 | mAP@10: 0.1074 | .5Acc+.5mAP: 0.0891 | CMC@10: 0.2477
[09-16 01:38] - Epoch  7: best loss improved from 27.5698 to 27.5119
[09-16 01:38] - Epoch 8 | lr 8.63e-02
[09-16 01:48] - Train loss: 27.2945 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 01:48] - Val   loss: 27.3656 | Acc_@1: 0.0832 | mAP@10: 0.1260 | .5Acc+.5mAP: 0.1046 | CMC@10: 0.2811
[09-16 01:48] - Epoch  8: best loss improved from 27.5119 to 27.3656
[09-16 01:48] - Epoch 9 | lr 8.43e-02
[09-16 02:03] - Train loss: 27.1402 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:03] - Val   loss: 27.2852 | Acc_@1: 0.1076 | mAP@10: 0.1524 | .5Acc+.5mAP: 0.1300 | CMC@10: 0.3164
[09-16 02:03] - Epoch  9: best loss improved from 27.3656 to 27.2852
[09-16 02:03] - Epoch 10 | lr 8.23e-02
[09-16 02:15] - Train loss: 26.9526 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:15] - Val   loss: 27.1070 | Acc_@1: 0.1328 | mAP@10: 0.1757 | .5Acc+.5mAP: 0.1543 | CMC@10: 0.3517
[09-16 02:15] - Epoch 10: best loss improved from 27.2852 to 27.1070
[09-16 02:15] - Epoch 11 | lr 8.03e-02
[09-16 02:25] - Train loss: 26.7583 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:25] - Val   loss: 27.0715 | Acc_@1: 0.1645 | mAP@10: 0.2051 | .5Acc+.5mAP: 0.1848 | CMC@10: 0.3945
[09-16 02:25] - Epoch 11: best loss improved from 27.1070 to 27.0715
[09-16 02:25] - Epoch 12 | lr 7.83e-02
[09-16 02:35] - Train loss: 26.5562 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:35] - Val   loss: 27.0943 | Acc_@1: 0.1634 | mAP@10: 0.2114 | .5Acc+.5mAP: 0.1874 | CMC@10: 0.4147
[09-16 02:35] - Epoch 13 | lr 7.63e-02
[09-16 02:45] - Train loss: 26.3466 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:45] - Val   loss: 26.7526 | Acc_@1: 0.1998 | mAP@10: 0.2510 | .5Acc+.5mAP: 0.2254 | CMC@10: 0.4766
[09-16 02:45] - Epoch 13: best loss improved from 27.0715 to 26.7526
[09-16 02:45] - Epoch 14 | lr 7.43e-02
[09-16 02:54] - Train loss: 26.1289 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 02:54] - Val   loss: 26.7797 | Acc_@1: 0.2397 | mAP@10: 0.2771 | .5Acc+.5mAP: 0.2584 | CMC@10: 0.4917
[09-16 02:54] - Epoch 15 | lr 7.23e-02
[09-16 03:05] - Train loss: 25.8803 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:05] - Val   loss: 26.6231 | Acc_@1: 0.2603 | mAP@10: 0.3058 | .5Acc+.5mAP: 0.2831 | CMC@10: 0.5349
[09-16 03:05] - Epoch 15: best loss improved from 26.7526 to 26.6231
[09-16 03:05] - Epoch 16 | lr 7.03e-02
[09-16 03:15] - Train loss: 25.6160 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:15] - Val   loss: 26.6027 | Acc_@1: 0.2703 | mAP@10: 0.3092 | .5Acc+.5mAP: 0.2898 | CMC@10: 0.5295
[09-16 03:15] - Epoch 16: best loss improved from 26.6231 to 26.6027
[09-16 03:15] - Epoch 17 | lr 6.83e-02
[09-16 03:26] - Train loss: 25.3384 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:26] - Val   loss: 26.4943 | Acc_@1: 0.2991 | mAP@10: 0.3398 | .5Acc+.5mAP: 0.3195 | CMC@10: 0.5641
[09-16 03:26] - Epoch 17: best loss improved from 26.6027 to 26.4943
[09-16 03:26] - Epoch 18 | lr 6.63e-02
[09-16 03:36] - Train loss: 25.0423 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:36] - Val   loss: 26.4889 | Acc_@1: 0.3294 | mAP@10: 0.3710 | .5Acc+.5mAP: 0.3502 | CMC@10: 0.6026
[09-16 03:36] - Epoch 18: best loss improved from 26.4943 to 26.4889
[09-16 03:36] - Epoch 19 | lr 6.43e-02
[09-16 03:47] - Train loss: 24.7245 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:47] - Val   loss: 26.6934 | Acc_@1: 0.3081 | mAP@10: 0.3530 | .5Acc+.5mAP: 0.3306 | CMC@10: 0.5860
[09-16 03:47] - Epoch 20 | lr 6.23e-02
[09-16 03:57] - Train loss: 24.3763 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 03:57] - Val   loss: 26.4430 | Acc_@1: 0.3589 | mAP@10: 0.3896 | .5Acc+.5mAP: 0.3742 | CMC@10: 0.6120
[09-16 03:57] - Epoch 20: best loss improved from 26.4889 to 26.4430
[09-16 03:57] - Epoch 21 | lr 6.03e-02
[09-16 04:06] - Train loss: 24.0127 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:06] - Val   loss: 26.2634 | Acc_@1: 0.3949 | mAP@10: 0.4279 | .5Acc+.5mAP: 0.4114 | CMC@10: 0.6587
[09-16 04:06] - Epoch 21: best loss improved from 26.4430 to 26.2634
[09-16 04:06] - Epoch 22 | lr 5.83e-02
[09-16 04:17] - Train loss: 23.6468 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:17] - Val   loss: 26.2148 | Acc_@1: 0.3672 | mAP@10: 0.4033 | .5Acc+.5mAP: 0.3852 | CMC@10: 0.6343
[09-16 04:17] - Epoch 22: best loss improved from 26.2634 to 26.2148
[09-16 04:17] - Epoch 23 | lr 5.63e-02
[09-16 04:27] - Train loss: 23.2289 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:27] - Val   loss: 26.0452 | Acc_@1: 0.4251 | mAP@10: 0.4499 | .5Acc+.5mAP: 0.4375 | CMC@10: 0.6807
[09-16 04:27] - Epoch 23: best loss improved from 26.2148 to 26.0452
[09-16 04:27] - Epoch 24 | lr 5.43e-02
[09-16 04:38] - Train loss: 22.8212 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:38] - Val   loss: 26.1302 | Acc_@1: 0.4474 | mAP@10: 0.4736 | .5Acc+.5mAP: 0.4605 | CMC@10: 0.6994
[09-16 04:38] - Epoch 25 | lr 5.23e-02
[09-16 04:48] - Train loss: 22.4002 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:48] - Val   loss: 25.9361 | Acc_@1: 0.4687 | mAP@10: 0.4899 | .5Acc+.5mAP: 0.4793 | CMC@10: 0.7185
[09-16 04:48] - Epoch 25: best loss improved from 26.0452 to 25.9361
[09-16 04:48] - Epoch 26 | lr 5.03e-02
[09-16 04:59] - Train loss: 21.9412 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 04:59] - Val   loss: 25.9197 | Acc_@1: 0.4730 | mAP@10: 0.4972 | .5Acc+.5mAP: 0.4851 | CMC@10: 0.7196
[09-16 04:59] - Epoch 26: best loss improved from 25.9361 to 25.9197
[09-16 04:59] - Epoch 27 | lr 4.83e-02
[09-16 05:17] - Train loss: 21.4893 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 05:17] - Val   loss: 25.8463 | Acc_@1: 0.4766 | mAP@10: 0.4924 | .5Acc+.5mAP: 0.4845 | CMC@10: 0.7124
[09-16 05:17] - Epoch 27: best loss improved from 25.9197 to 25.8463
[09-16 05:17] - Epoch 28 | lr 4.63e-02
[09-16 05:36] - Train loss: 21.0190 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 05:36] - Val   loss: 25.7382 | Acc_@1: 0.5047 | mAP@10: 0.5217 | .5Acc+.5mAP: 0.5132 | CMC@10: 0.7423
[09-16 05:36] - Epoch 28: best loss improved from 25.8463 to 25.7382
[09-16 05:36] - Epoch 29 | lr 4.43e-02
[09-16 05:54] - Train loss: 20.5251 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 05:54] - Val   loss: 25.9132 | Acc_@1: 0.4975 | mAP@10: 0.5174 | .5Acc+.5mAP: 0.5074 | CMC@10: 0.7369
[09-16 05:54] - Epoch 30 | lr 4.23e-02
[09-16 06:13] - Train loss: 20.0691 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 06:13] - Val   loss: 25.3893 | Acc_@1: 0.5263 | mAP@10: 0.5479 | .5Acc+.5mAP: 0.5371 | CMC@10: 0.7552
[09-16 06:13] - Epoch 30: best loss improved from 25.7382 to 25.3893
[09-16 06:13] - Epoch 31 | lr 4.03e-02
[09-16 06:33] - Train loss: 19.5637 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 06:33] - Val   loss: 25.3595 | Acc_@1: 0.5446 | mAP@10: 0.5596 | .5Acc+.5mAP: 0.5521 | CMC@10: 0.7696
[09-16 06:33] - Epoch 31: best loss improved from 25.3893 to 25.3595
[09-16 06:33] - Epoch 32 | lr 3.83e-02
[09-16 06:52] - Train loss: 19.1193 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 06:52] - Val   loss: 26.0158 | Acc_@1: 0.5378 | mAP@10: 0.5477 | .5Acc+.5mAP: 0.5428 | CMC@10: 0.7538
[09-16 06:52] - Epoch 33 | lr 3.63e-02
[09-16 07:12] - Train loss: 18.5992 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 07:12] - Val   loss: 25.1023 | Acc_@1: 0.5814 | mAP@10: 0.5902 | .5Acc+.5mAP: 0.5858 | CMC@10: 0.7948
[09-16 07:12] - Epoch 33: best loss improved from 25.3595 to 25.1023
[09-16 07:12] - Epoch 34 | lr 3.43e-02
[09-16 07:31] - Train loss: 18.1426 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 07:31] - Val   loss: 24.6984 | Acc_@1: 0.6098 | mAP@10: 0.6158 | .5Acc+.5mAP: 0.6128 | CMC@10: 0.8121
[09-16 07:31] - Epoch 34: best loss improved from 25.1023 to 24.6984
[09-16 07:31] - Epoch 35 | lr 3.23e-02
[09-16 07:50] - Train loss: 17.6602 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 07:50] - Val   loss: 24.8514 | Acc_@1: 0.5896 | mAP@10: 0.6025 | .5Acc+.5mAP: 0.5961 | CMC@10: 0.7977
[09-16 07:50] - Epoch 36 | lr 3.03e-02
[09-16 08:10] - Train loss: 17.1655 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 08:10] - Val   loss: 24.5899 | Acc_@1: 0.5954 | mAP@10: 0.6027 | .5Acc+.5mAP: 0.5990 | CMC@10: 0.7937
[09-16 08:10] - Epoch 36: best loss improved from 24.6984 to 24.5899
[09-16 08:10] - Epoch 37 | lr 2.83e-02
[09-16 08:29] - Train loss: 16.7009 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 08:29] - Val   loss: 24.6001 | Acc_@1: 0.5871 | mAP@10: 0.5925 | .5Acc+.5mAP: 0.5898 | CMC@10: 0.7811
[09-16 08:29] - Epoch 38 | lr 2.63e-02
[09-16 08:48] - Train loss: 16.2206 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 08:48] - Val   loss: 24.6372 | Acc_@1: 0.6123 | mAP@10: 0.6166 | .5Acc+.5mAP: 0.6144 | CMC@10: 0.7988
[09-16 08:48] - Epoch 39 | lr 2.43e-02
[09-16 09:07] - Train loss: 15.7493 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 09:07] - Val   loss: 24.2817 | Acc_@1: 0.6235 | mAP@10: 0.6348 | .5Acc+.5mAP: 0.6291 | CMC@10: 0.8279
[09-16 09:07] - Epoch 39: best loss improved from 24.5899 to 24.2817
[09-16 09:07] - Epoch 40 | lr 2.23e-02
[09-16 09:25] - Train loss: 15.2808 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 09:25] - Val   loss: 23.9416 | Acc_@1: 0.6321 | mAP@10: 0.6375 | .5Acc+.5mAP: 0.6348 | CMC@10: 0.8211
[09-16 09:25] - Epoch 40: best loss improved from 24.2817 to 23.9416
[09-16 09:25] - Epoch 41 | lr 2.03e-02
[09-16 09:42] - Train loss: 14.8195 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 09:42] - Val   loss: 23.6679 | Acc_@1: 0.6577 | mAP@10: 0.6592 | .5Acc+.5mAP: 0.6584 | CMC@10: 0.8362
[09-16 09:42] - Epoch 41: best loss improved from 23.9416 to 23.6679
[09-16 09:42] - Epoch 42 | lr 1.83e-02
[09-16 10:00] - Train loss: 14.3434 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 10:00] - Val   loss: 23.7637 | Acc_@1: 0.6569 | mAP@10: 0.6617 | .5Acc+.5mAP: 0.6593 | CMC@10: 0.8470
[09-16 10:00] - Epoch 43 | lr 1.63e-02
[09-16 10:17] - Train loss: 13.8828 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 10:17] - Val   loss: 23.3491 | Acc_@1: 0.6807 | mAP@10: 0.6746 | .5Acc+.5mAP: 0.6776 | CMC@10: 0.8477
[09-16 10:17] - Epoch 43: best loss improved from 23.6679 to 23.3491
[09-16 10:17] - Epoch 44 | lr 1.43e-02
[09-16 10:35] - Train loss: 13.4434 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 10:35] - Val   loss: 23.2536 | Acc_@1: 0.6587 | mAP@10: 0.6621 | .5Acc+.5mAP: 0.6604 | CMC@10: 0.8449
[09-16 10:35] - Epoch 44: best loss improved from 23.3491 to 23.2536
[09-16 10:35] - Epoch 45 | lr 1.23e-02
[09-16 10:56] - Train loss: 13.0044 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 10:56] - Val   loss: 23.0656 | Acc_@1: 0.6735 | mAP@10: 0.6793 | .5Acc+.5mAP: 0.6764 | CMC@10: 0.8531
[09-16 10:56] - Epoch 45: best loss improved from 23.2536 to 23.0656
[09-16 10:56] - Epoch 46 | lr 1.03e-02
[09-16 11:08] - Train loss: 12.5648 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:08] - Val   loss: 22.7381 | Acc_@1: 0.6757 | mAP@10: 0.6836 | .5Acc+.5mAP: 0.6796 | CMC@10: 0.8625
[09-16 11:08] - Epoch 46: best loss improved from 23.0656 to 22.7381
[09-16 11:08] - Epoch 47 | lr 8.28e-03
[09-16 11:17] - Train loss: 12.1390 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:17] - Val   loss: 22.7195 | Acc_@1: 0.6955 | mAP@10: 0.6944 | .5Acc+.5mAP: 0.6949 | CMC@10: 0.8722
[09-16 11:17] - Epoch 47: best loss improved from 22.7381 to 22.7195
[09-16 11:17] - Epoch 48 | lr 6.28e-03
[09-16 11:27] - Train loss: 11.7526 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:27] - Val   loss: 22.5997 | Acc_@1: 0.6940 | mAP@10: 0.6947 | .5Acc+.5mAP: 0.6944 | CMC@10: 0.8672
[09-16 11:27] - Epoch 48: best loss improved from 22.7195 to 22.5997
[09-16 11:27] - Epoch 49 | lr 4.28e-03
[09-16 11:36] - Train loss: 11.3730 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:36] - Val   loss: 22.5178 | Acc_@1: 0.7009 | mAP@10: 0.7011 | .5Acc+.5mAP: 0.7010 | CMC@10: 0.8668
[09-16 11:36] - Epoch 49: best loss improved from 22.5997 to 22.5178
[09-16 11:36] - Epoch 50 | lr 2.28e-03
