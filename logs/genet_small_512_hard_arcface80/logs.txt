[09-20 05:13] - Parameters used for training: Namespace(arch='genet_small', augmentation='hard', batch_size=96, config_file='configs/2_genet_small_512_finetune.yaml', criterion='arcface', criterion_params={'out_features': 3097, 's': 80.0, 'm': 1.0}, debug=False, ema_decay=0.0, embedding_size=512, freeze_bn=False, model_params={}, name='genet_small_512_hard_arcface80', optim='adamw', outdir='logs/genet_small_512_hard_arcface80', phases=[{'ep': [0, 30], 'lr': [0.001, 1e-05]}], pooling='gem', resume='logs/genet_small_512_light_arcface80/model.chpn', root='data/interim', seed=42, size=512, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=8)
[09-20 05:14] - Model size: 7.24M
[09-20 05:14] - Loss for this run is: AdditiveAngularMarginLoss(
  (criterion): CrossEntropyLoss()
)
[09-20 05:14] - Using sizes {(680, 512), (512, 512), (1024, 512), (512, 904), (512, 640), (640, 512), (512, 768), (904, 512), (512, 680), (512, 1024), (768, 512)} for train
[09-20 05:14] - Using sizes {(768, 1360), (960, 768), (1536, 768), (768, 768), (1360, 768), (1152, 768), (768, 1536), (1024, 768), (768, 960), (768, 1024), (768, 1152)} for validation
[09-20 05:14] - Val size: 16672
[09-20 05:14] - Train size: 51935
[09-20 05:14] - Start training
[09-20 05:14] - Start phase #1 from epoch 0 to epoch 30: {'ep': [0, 30], 'lr': [0.001, 1e-05], 'mom': [], 'mode': 'linear'}
[09-20 05:14] - Epoch 1 | lr 0.00e+00
[09-20 05:18] - TimeMeter profiling. Data time: 2.80E-01s. Model time: 6.57E-01s 

[09-20 05:22] - Train loss: 38.7948 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 05:22] - Val   loss: 92.3216 | Acc@1: 0.8345 | mAP@10: 0.8240 | target: 0.8293 | mAP@R: 0.7910
[09-20 05:22] - Epoch  1: best target improved from -inf to 0.8293
[09-20 05:22] - Epoch  1: best mAP@R improved from -inf to 0.7910
[09-20 05:22] - Epoch 2 | lr 9.72e-04
[09-20 05:33] - Train loss: 37.1636 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 05:33] - Val   loss: 92.1427 | Acc@1: 0.8521 | mAP@10: 0.8341 | target: 0.8431 | mAP@R: 0.8017
[09-20 05:33] - Epoch  2: best target improved from 0.8293 to 0.8431
[09-20 05:33] - Epoch  2: best mAP@R improved from 0.7910 to 0.8017
[09-20 05:33] - Epoch 3 | lr 9.39e-04
[09-20 05:43] - Train loss: 36.2488 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 05:43] - Val   loss: 92.2198 | Acc@1: 0.8512 | mAP@10: 0.8348 | target: 0.8430 | mAP@R: 0.8021
[09-20 05:43] - Epoch  3: best mAP@R improved from 0.8017 to 0.8021
[09-20 05:43] - Epoch 4 | lr 9.06e-04
[09-20 05:56] - Train loss: 35.4991 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 05:56] - Val   loss: 92.4222 | Acc@1: 0.8477 | mAP@10: 0.8273 | target: 0.8375 | mAP@R: 0.7972
[09-20 05:56] - Epoch 5 | lr 8.73e-04
[09-20 06:04] - Train loss: 35.0013 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 06:04] - Val   loss: 92.6802 | Acc@1: 0.8460 | mAP@10: 0.8374 | target: 0.8417 | mAP@R: 0.8065
[09-20 06:04] - Epoch  5: best mAP@R improved from 0.8021 to 0.8065
[09-20 06:04] - Epoch 6 | lr 8.40e-04
[09-20 06:13] - Train loss: 34.5595 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 06:13] - Val   loss: 92.8058 | Acc@1: 0.8495 | mAP@10: 0.8377 | target: 0.8436 | mAP@R: 0.8061
[09-20 06:13] - Epoch  6: best target improved from 0.8431 to 0.8436
[09-20 06:13] - Epoch 7 | lr 8.07e-04
[09-20 06:34] - Train loss: 34.1877 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 06:34] - Val   loss: 92.8035 | Acc@1: 0.8433 | mAP@10: 0.8342 | target: 0.8388 | mAP@R: 0.8032
[09-20 06:34] - Epoch 8 | lr 7.74e-04
