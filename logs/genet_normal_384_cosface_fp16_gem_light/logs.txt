[09-16 11:01] - Parameters used for training: Namespace(arch='genet_normal', augmentation='light', batch_size=96, config_file='configs/genet_normal.yaml', criterion='cosface', criterion_params={'out_features': 3097, 's': 30.0, 'm': 0.5}, debug=False, ema_decay=0.0, embedding_size=512, model_params={}, name='genet_normal_384_cosface_fp16_gem_light', optim='adamw', outdir='logs/genet_normal_384_cosface_fp16_gem_light', phases=[{'ep': [0, 50], 'lr': [0.1, 1e-05]}], pooling='gem', resume='', root='data/interim', seed=42, size=384, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=6)
[09-16 11:01] - Start training
[09-16 11:01] - Model size: 19.89M
[09-16 11:01] - Loss for this run is: LargeMarginCosineLoss(
  (criterion): CrossEntropyLoss()
)
[09-16 11:01] - Val size: 13761
[09-16 11:01] - Train size: 68805
[09-16 11:01] - Start phase #1 from epoch 0 to epoch 50: {'ep': [0, 50], 'lr': [0.1, 1e-05], 'mom': [], 'mode': 'linear'}
[09-16 11:01] - Epoch 1 | lr 0.00e+00
[09-16 11:04] - 
TimeMeter profiling. Data time: 7.98E-04s. Model time: 2.51E-01s 

[09-16 11:07] - Train loss: 21.4512 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:07] - Val   loss: 21.6127 | Acc_@1: 0.5633 | mAP@10: 0.5723 | .5Acc+.5mAP: 0.5678 | CMC@10: 0.7739
[09-16 11:07] - Epoch  1: best loss improved from inf to 21.6127
[09-16 11:07] - Epoch 2 | lr 9.86e-02
[09-16 11:11] - Train loss: 16.8514 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:11] - Val   loss: 20.7123 | Acc_@1: 0.6288 | mAP@10: 0.6311 | .5Acc+.5mAP: 0.6300 | CMC@10: 0.8100
[09-16 11:11] - Epoch  2: best loss improved from 21.6127 to 20.7123
[09-16 11:11] - Epoch 3 | lr 9.66e-02
[09-16 11:16] - Train loss: 13.5584 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:16] - Val   loss: 19.8512 | Acc_@1: 0.6288 | mAP@10: 0.6395 | .5Acc+.5mAP: 0.6342 | CMC@10: 0.8188
[09-16 11:16] - Epoch  3: best loss improved from 20.7123 to 19.8512
[09-16 11:16] - Epoch 4 | lr 9.46e-02
[09-16 11:21] - Train loss: 11.2250 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:21] - Val   loss: 18.9110 | Acc_@1: 0.7102 | mAP@10: 0.7138 | .5Acc+.5mAP: 0.7120 | CMC@10: 0.8790
[09-16 11:21] - Epoch  4: best loss improved from 19.8512 to 18.9110
[09-16 11:21] - Epoch 5 | lr 9.26e-02
[09-16 11:25] - Train loss: 9.5385 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:25] - Val   loss: 18.3073 | Acc_@1: 0.7505 | mAP@10: 0.7477 | .5Acc+.5mAP: 0.7491 | CMC@10: 0.8868
[09-16 11:25] - Epoch  5: best loss improved from 18.9110 to 18.3073
[09-16 11:25] - Epoch 6 | lr 9.06e-02
[09-16 11:30] - Train loss: 8.2741 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:30] - Val   loss: 18.1066 | Acc_@1: 0.7321 | mAP@10: 0.7281 | .5Acc+.5mAP: 0.7301 | CMC@10: 0.8726
[09-16 11:30] - Epoch  6: best loss improved from 18.3073 to 18.1066
[09-16 11:30] - Epoch 7 | lr 8.86e-02
[09-16 11:35] - Train loss: 7.2283 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:35] - Val   loss: 17.0392 | Acc_@1: 0.7824 | mAP@10: 0.7746 | .5Acc+.5mAP: 0.7785 | CMC@10: 0.9027
[09-16 11:35] - Epoch  7: best loss improved from 18.1066 to 17.0392
[09-16 11:35] - Epoch 8 | lr 8.66e-02
[09-16 11:42] - Train loss: 6.4111 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:42] - Val   loss: 16.2569 | Acc_@1: 0.7817 | mAP@10: 0.7814 | .5Acc+.5mAP: 0.7815 | CMC@10: 0.8974
[09-16 11:42] - Epoch  8: best loss improved from 17.0392 to 16.2569
[09-16 11:42] - Epoch 9 | lr 8.46e-02
[09-16 11:47] - Train loss: 5.6956 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:47] - Val   loss: 15.2265 | Acc_@1: 0.8025 | mAP@10: 0.7992 | .5Acc+.5mAP: 0.8009 | CMC@10: 0.9172
[09-16 11:47] - Epoch  9: best loss improved from 16.2569 to 15.2265
[09-16 11:47] - Epoch 10 | lr 8.26e-02
[09-16 11:51] - Train loss: 5.1175 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:51] - Val   loss: 15.5549 | Acc_@1: 0.7795 | mAP@10: 0.7817 | .5Acc+.5mAP: 0.7806 | CMC@10: 0.8999
[09-16 11:51] - Epoch 11 | lr 8.06e-02
[09-16 11:56] - Train loss: 4.6961 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:56] - Val   loss: 15.0708 | Acc_@1: 0.8093 | mAP@10: 0.8077 | .5Acc+.5mAP: 0.8085 | CMC@10: 0.9197
[09-16 11:56] - Epoch 11: best loss improved from 15.2265 to 15.0708
[09-16 11:56] - Epoch 12 | lr 7.86e-02
[09-16 12:00] - Train loss: 4.3669 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:00] - Val   loss: 15.5299 | Acc_@1: 0.7873 | mAP@10: 0.7839 | .5Acc+.5mAP: 0.7856 | CMC@10: 0.8967
[09-16 12:00] - Epoch 13 | lr 7.66e-02
[09-16 12:05] - Train loss: 3.9574 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:05] - Val   loss: 15.0372 | Acc_@1: 0.8011 | mAP@10: 0.7975 | .5Acc+.5mAP: 0.7993 | CMC@10: 0.9023
[09-16 12:05] - Epoch 13: best loss improved from 15.0708 to 15.0372
[09-16 12:05] - Epoch 14 | lr 7.46e-02
[09-16 12:10] - Train loss: 3.5845 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:10] - Val   loss: 14.4990 | Acc_@1: 0.8022 | mAP@10: 0.7980 | .5Acc+.5mAP: 0.8001 | CMC@10: 0.9027
[09-16 12:10] - Epoch 14: best loss improved from 15.0372 to 14.4990
[09-16 12:10] - Epoch 15 | lr 7.26e-02
[09-16 12:14] - Train loss: 3.3175 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:14] - Val   loss: 14.8240 | Acc_@1: 0.7870 | mAP@10: 0.7834 | .5Acc+.5mAP: 0.7852 | CMC@10: 0.8836
[09-16 12:14] - Epoch 16 | lr 7.06e-02
[09-16 12:19] - Train loss: 3.0783 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:19] - Val   loss: 13.6572 | Acc_@1: 0.8199 | mAP@10: 0.8164 | .5Acc+.5mAP: 0.8182 | CMC@10: 0.9091
[09-16 12:19] - Epoch 16: best loss improved from 14.4990 to 13.6572
[09-16 12:19] - Epoch 17 | lr 6.86e-02
[09-16 12:23] - Train loss: 2.8386 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:23] - Val   loss: 13.4566 | Acc_@1: 0.8263 | mAP@10: 0.8197 | .5Acc+.5mAP: 0.8230 | CMC@10: 0.9112
[09-16 12:23] - Epoch 17: best loss improved from 13.6572 to 13.4566
[09-16 12:23] - Epoch 18 | lr 6.66e-02
[09-16 12:28] - Train loss: 2.6595 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:28] - Val   loss: 13.5371 | Acc_@1: 0.8174 | mAP@10: 0.8172 | .5Acc+.5mAP: 0.8173 | CMC@10: 0.9140
[09-16 12:28] - Epoch 19 | lr 6.46e-02
[09-16 12:32] - Train loss: 2.4733 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:32] - Val   loss: 12.8654 | Acc_@1: 0.8220 | mAP@10: 0.8200 | .5Acc+.5mAP: 0.8210 | CMC@10: 0.9176
[09-16 12:32] - Epoch 19: best loss improved from 13.4566 to 12.8654
[09-16 12:32] - Epoch 20 | lr 6.26e-02
[09-16 12:37] - Train loss: 2.3408 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:37] - Val   loss: 13.6837 | Acc_@1: 0.7990 | mAP@10: 0.7927 | .5Acc+.5mAP: 0.7958 | CMC@10: 0.8836
[09-16 12:37] - Epoch 21 | lr 6.06e-02
[09-16 12:41] - Train loss: 2.1708 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:41] - Val   loss: 14.9180 | Acc_@1: 0.7449 | mAP@10: 0.7443 | .5Acc+.5mAP: 0.7446 | CMC@10: 0.8454
[09-16 12:41] - Epoch 22 | lr 5.86e-02
[09-16 12:46] - Train loss: 2.0189 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:46] - Val   loss: 14.0789 | Acc_@1: 0.7866 | mAP@10: 0.7884 | .5Acc+.5mAP: 0.7875 | CMC@10: 0.8815
[09-16 12:46] - Epoch 23 | lr 5.66e-02
[09-16 12:51] - Train loss: 1.8928 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:51] - Val   loss: 13.6738 | Acc_@1: 0.7916 | mAP@10: 0.7926 | .5Acc+.5mAP: 0.7921 | CMC@10: 0.8864
[09-16 12:51] - Epoch 24 | lr 5.46e-02
[09-16 12:55] - Train loss: 1.7606 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:55] - Val   loss: 12.7908 | Acc_@1: 0.8252 | mAP@10: 0.8220 | .5Acc+.5mAP: 0.8236 | CMC@10: 0.9045
[09-16 12:55] - Epoch 24: best loss improved from 12.8654 to 12.7908
[09-16 12:55] - Epoch 25 | lr 5.26e-02
[09-16 13:00] - Train loss: 1.6526 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:00] - Val   loss: 13.2231 | Acc_@1: 0.7941 | mAP@10: 0.7954 | .5Acc+.5mAP: 0.7947 | CMC@10: 0.8931
[09-16 13:00] - Epoch 26 | lr 5.06e-02
[09-16 13:04] - Train loss: 1.5618 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:04] - Val   loss: 12.6359 | Acc_@1: 0.7962 | mAP@10: 0.7999 | .5Acc+.5mAP: 0.7980 | CMC@10: 0.8953
[09-16 13:04] - Epoch 26: best loss improved from 12.7908 to 12.6359
[09-16 13:04] - Epoch 27 | lr 4.86e-02
[09-16 13:09] - Train loss: 1.4861 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:09] - Val   loss: 12.7467 | Acc_@1: 0.8071 | mAP@10: 0.8075 | .5Acc+.5mAP: 0.8073 | CMC@10: 0.8903
[09-16 13:09] - Epoch 28 | lr 4.66e-02
[09-16 13:13] - Train loss: 1.4070 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:13] - Val   loss: 12.5733 | Acc_@1: 0.8245 | mAP@10: 0.8216 | .5Acc+.5mAP: 0.8230 | CMC@10: 0.9105
[09-16 13:13] - Epoch 28: best loss improved from 12.6359 to 12.5733
[09-16 13:13] - Epoch 29 | lr 4.46e-02
[09-16 13:18] - Train loss: 1.3167 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:18] - Val   loss: 12.6794 | Acc_@1: 0.8110 | mAP@10: 0.8096 | .5Acc+.5mAP: 0.8103 | CMC@10: 0.8967
[09-16 13:18] - Epoch 30 | lr 4.26e-02
[09-16 13:23] - Train loss: 1.2503 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:23] - Val   loss: 11.8976 | Acc_@1: 0.8379 | mAP@10: 0.8319 | .5Acc+.5mAP: 0.8349 | CMC@10: 0.9137
[09-16 13:23] - Epoch 30: best loss improved from 12.5733 to 11.8976
[09-16 13:23] - Epoch 31 | lr 4.06e-02
[09-16 13:27] - Train loss: 1.1869 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:27] - Val   loss: 12.8249 | Acc_@1: 0.8061 | mAP@10: 0.8091 | .5Acc+.5mAP: 0.8076 | CMC@10: 0.8949
[09-16 13:27] - Epoch 32 | lr 3.86e-02
[09-16 13:32] - Train loss: 1.1264 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:32] - Val   loss: 12.0314 | Acc_@1: 0.8270 | mAP@10: 0.8214 | .5Acc+.5mAP: 0.8242 | CMC@10: 0.9038
[09-16 13:32] - Epoch 33 | lr 3.66e-02
[09-16 13:36] - Train loss: 1.0673 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:36] - Val   loss: 12.1397 | Acc_@1: 0.8139 | mAP@10: 0.8106 | .5Acc+.5mAP: 0.8122 | CMC@10: 0.8974
[09-16 13:36] - Epoch 34 | lr 3.46e-02
[09-16 13:41] - Train loss: 1.0145 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:41] - Val   loss: 11.2856 | Acc_@1: 0.8496 | mAP@10: 0.8487 | .5Acc+.5mAP: 0.8492 | CMC@10: 0.9289
[09-16 13:41] - Epoch 34: best loss improved from 11.8976 to 11.2856
[09-16 13:41] - Epoch 35 | lr 3.26e-02
[09-16 13:46] - Train loss: 0.9688 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:46] - Val   loss: 11.6808 | Acc_@1: 0.8241 | mAP@10: 0.8269 | .5Acc+.5mAP: 0.8255 | CMC@10: 0.9066
[09-16 13:46] - Epoch 36 | lr 3.06e-02
[09-16 13:50] - Train loss: 0.9283 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:50] - Val   loss: 11.8300 | Acc_@1: 0.8224 | mAP@10: 0.8249 | .5Acc+.5mAP: 0.8237 | CMC@10: 0.9066
[09-16 13:50] - Epoch 37 | lr 2.86e-02
[09-16 13:55] - Train loss: 0.8928 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:55] - Val   loss: 11.7665 | Acc_@1: 0.8270 | mAP@10: 0.8244 | .5Acc+.5mAP: 0.8257 | CMC@10: 0.9045
[09-16 13:55] - Epoch 38 | lr 2.66e-02
[09-16 13:59] - Train loss: 0.8636 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:59] - Val   loss: 11.8424 | Acc_@1: 0.8181 | mAP@10: 0.8160 | .5Acc+.5mAP: 0.8171 | CMC@10: 0.8970
[09-16 13:59] - Epoch 39 | lr 2.46e-02
[09-16 14:04] - Train loss: 0.8338 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:04] - Val   loss: 11.3489 | Acc_@1: 0.8355 | mAP@10: 0.8344 | .5Acc+.5mAP: 0.8349 | CMC@10: 0.9147
[09-16 14:04] - Epoch 40 | lr 2.26e-02
[09-16 14:08] - Train loss: 0.8177 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:08] - Val   loss: 10.7381 | Acc_@1: 0.8500 | mAP@10: 0.8474 | .5Acc+.5mAP: 0.8487 | CMC@10: 0.9282
[09-16 14:08] - Epoch 40: best loss improved from 11.2856 to 10.7381
[09-16 14:08] - Epoch 41 | lr 2.06e-02
[09-16 14:13] - Train loss: 0.7849 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:13] - Val   loss: 11.0317 | Acc_@1: 0.8408 | mAP@10: 0.8382 | .5Acc+.5mAP: 0.8395 | CMC@10: 0.9172
[09-16 14:13] - Epoch 42 | lr 1.86e-02
[09-16 14:18] - Train loss: 0.7709 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:18] - Val   loss: 10.9339 | Acc_@1: 0.8422 | mAP@10: 0.8452 | .5Acc+.5mAP: 0.8437 | CMC@10: 0.9236
[09-16 14:18] - Epoch 43 | lr 1.66e-02
[09-16 14:22] - Train loss: 0.7542 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:22] - Val   loss: 10.8172 | Acc_@1: 0.8563 | mAP@10: 0.8526 | .5Acc+.5mAP: 0.8545 | CMC@10: 0.9282
[09-16 14:22] - Epoch 44 | lr 1.46e-02
[09-16 14:27] - Train loss: 0.7384 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:27] - Val   loss: 10.9878 | Acc_@1: 0.8383 | mAP@10: 0.8403 | .5Acc+.5mAP: 0.8393 | CMC@10: 0.9151
[09-16 14:27] - Epoch 45 | lr 1.26e-02
[09-16 14:31] - Train loss: 0.7305 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:31] - Val   loss: 10.6189 | Acc_@1: 0.8585 | mAP@10: 0.8552 | .5Acc+.5mAP: 0.8568 | CMC@10: 0.9292
[09-16 14:31] - Epoch 45: best loss improved from 10.7381 to 10.6189
[09-16 14:31] - Epoch 46 | lr 1.06e-02
[09-16 14:36] - Train loss: 0.7169 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:36] - Val   loss: 10.5769 | Acc_@1: 0.8542 | mAP@10: 0.8527 | .5Acc+.5mAP: 0.8535 | CMC@10: 0.9250
[09-16 14:36] - Epoch 46: best loss improved from 10.6189 to 10.5769
[09-16 14:36] - Epoch 47 | lr 8.62e-03
[09-16 14:40] - Train loss: 0.7143 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:40] - Val   loss: 10.5002 | Acc_@1: 0.8546 | mAP@10: 0.8507 | .5Acc+.5mAP: 0.8526 | CMC@10: 0.9232
[09-16 14:40] - Epoch 47: best loss improved from 10.5769 to 10.5002
[09-16 14:40] - Epoch 48 | lr 6.62e-03
[09-16 14:45] - Train loss: 0.6926 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:45] - Val   loss: 10.4192 | Acc_@1: 0.8577 | mAP@10: 0.8574 | .5Acc+.5mAP: 0.8576 | CMC@10: 0.9264
[09-16 14:45] - Epoch 48: best loss improved from 10.5002 to 10.4192
[09-16 14:45] - Epoch 49 | lr 4.62e-03
[09-16 14:50] - Train loss: 0.6925 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:50] - Val   loss: 10.3939 | Acc_@1: 0.8585 | mAP@10: 0.8575 | .5Acc+.5mAP: 0.8580 | CMC@10: 0.9268
[09-16 14:50] - Epoch 49: best loss improved from 10.4192 to 10.3939
[09-16 14:50] - Epoch 50 | lr 2.62e-03
[09-16 14:54] - Train loss: 0.6901 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:54] - Val   loss: 10.1788 | Acc_@1: 0.8677 | mAP@10: 0.8668 | .5Acc+.5mAP: 0.8672 | CMC@10: 0.9370
[09-16 14:54] - Epoch 50: best loss improved from 10.3939 to 10.1788
[09-16 14:54] - Loading best model from previous phase
[09-16 14:54] - Finished Training. Took: 233.59m
