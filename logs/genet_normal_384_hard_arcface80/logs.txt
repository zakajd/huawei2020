[09-20 09:04] - Parameters used for training: Namespace(arch='genet_normal', augmentation='hard', batch_size=96, config_file='configs/2_genet_normal_finetune.yaml', criterion='arcface', criterion_params={'out_features': 3097, 's': 80.0, 'm': 1.0}, debug=False, ema_decay=0.0, embedding_size=512, freeze_bn=False, model_params={}, name='genet_normal_384_hard_arcface80', optim='adamw', outdir='logs/genet_normal_384_hard_arcface80', phases=[{'ep': [0, 1], 'lr': [1e-06, 0.001]}, {'ep': [1, 30], 'lr': [0.001, 1e-05]}], pooling='gem', resume='logs/genet_normal_384_light_arcface80/model.chpn', root='data/interim', seed=42, size=384, tta=False, use_fp16=True, val_frequency=1, val_size=512, weight_decay=1e-05, workers=6)
[09-20 09:04] - Model size: 19.89M
[09-20 09:04] - Loss for this run is: AdditiveAngularMarginLoss(
  (criterion): CrossEntropyLoss()
)
[09-20 09:04] - Using sizes {(384, 768), (384, 512), (480, 384), (384, 384), (512, 384), (384, 576), (384, 480), (768, 384), (576, 384), (680, 384), (384, 680)} for train
[09-20 09:04] - Using sizes {(768, 512), (512, 512), (640, 512), (512, 1024), (512, 768), (1024, 512), (512, 640), (680, 512), (904, 512), (512, 904), (512, 680)} for validation
[09-20 09:04] - Val size: 16656
[09-20 09:04] - Train size: 51945
[09-20 09:04] - Start training
[09-20 09:04] - Start phase #1 from epoch 0 to epoch 1: {'ep': [0, 1], 'lr': [1e-06, 0.001], 'mom': [], 'mode': 'linear'}
[09-20 09:04] - Epoch 1 | lr 0.00e+00
[09-20 09:09] - 
TimeMeter profiling. Data time: 2.29E-03s. Model time: 5.84E-01s 

[09-20 09:11] - Train loss: 26.4735 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:11] - Val   loss: 97.5968 | Acc@1: 0.8924 | mAP@10: 0.8660 | target: 0.8792 | mAP@R: 0.8410
[09-20 09:11] - Epoch  1: best target improved from -inf to 0.8792
[09-20 09:11] - Epoch  1: best mAP@R improved from -inf to 0.8410
[09-20 09:11] - Loading best model from previous phase
[09-20 09:11] - Start phase #2 from epoch 1 to epoch 30: {'ep': [1, 30], 'lr': [0.001, 1e-05], 'mom': [], 'mode': 'linear'}
[09-20 09:11] - Epoch 2 | lr 8.38e-04
[09-20 09:18] - Train loss: 25.3988 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:18] - Val   loss: 97.9856 | Acc@1: 0.8854 | mAP@10: 0.8693 | target: 0.8773 | mAP@R: 0.8438
[09-20 09:18] - Epoch  2: best mAP@R improved from 0.8410 to 0.8438
[09-20 09:18] - Epoch 3 | lr 9.71e-04
[09-20 09:26] - Train loss: 24.1427 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:26] - Val   loss: 98.4042 | Acc@1: 0.8907 | mAP@10: 0.8682 | target: 0.8794 | mAP@R: 0.8434
[09-20 09:26] - Epoch  3: best target improved from 0.8792 to 0.8794
[09-20 09:26] - Epoch 4 | lr 9.37e-04
[09-20 09:34] - Train loss: 23.0629 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:34] - Val   loss: 98.3094 | Acc@1: 0.8889 | mAP@10: 0.8690 | target: 0.8790 | mAP@R: 0.8415
[09-20 09:34] - Epoch 5 | lr 9.03e-04
[09-20 09:42] - Train loss: 22.1220 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:42] - Val   loss: 98.7759 | Acc@1: 0.8836 | mAP@10: 0.8713 | target: 0.8775 | mAP@R: 0.8453
[09-20 09:42] - Epoch  5: best mAP@R improved from 0.8438 to 0.8453
[09-20 09:42] - Epoch 6 | lr 8.69e-04
[09-20 09:52] - Train loss: 21.4295 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 09:52] - Val   loss: 98.8845 | Acc@1: 0.8951 | mAP@10: 0.8732 | target: 0.8842 | mAP@R: 0.8479
[09-20 09:52] - Epoch  6: best target improved from 0.8794 to 0.8842
[09-20 09:52] - Epoch  6: best mAP@R improved from 0.8453 to 0.8479
[09-20 09:52] - Epoch 7 | lr 8.35e-04
[09-20 10:01] - Train loss: 20.7547 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:01] - Val   loss: 98.4620 | Acc@1: 0.8915 | mAP@10: 0.8725 | target: 0.8820 | mAP@R: 0.8483
[09-20 10:01] - Epoch  7: best mAP@R improved from 0.8479 to 0.8483
[09-20 10:01] - Epoch 8 | lr 8.01e-04
[09-20 10:09] - Train loss: 20.1420 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:09] - Val   loss: 98.9907 | Acc@1: 0.8977 | mAP@10: 0.8761 | target: 0.8869 | mAP@R: 0.8499
[09-20 10:09] - Epoch  8: best target improved from 0.8842 to 0.8869
[09-20 10:09] - Epoch  8: best mAP@R improved from 0.8483 to 0.8499
[09-20 10:09] - Epoch 9 | lr 7.67e-04
[09-20 10:16] - Train loss: 19.6826 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:16] - Val   loss: 99.4542 | Acc@1: 0.8898 | mAP@10: 0.8739 | target: 0.8818 | mAP@R: 0.8458
[09-20 10:16] - Epoch 10 | lr 7.32e-04
[09-20 10:24] - Train loss: 19.2517 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:24] - Val   loss: 99.3827 | Acc@1: 0.8889 | mAP@10: 0.8721 | target: 0.8805 | mAP@R: 0.8464
[09-20 10:24] - Epoch 11 | lr 6.98e-04
[09-20 10:33] - Train loss: 18.7992 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:33] - Val   loss: 99.3595 | Acc@1: 0.8968 | mAP@10: 0.8754 | target: 0.8861 | mAP@R: 0.8520
[09-20 10:33] - Epoch 11: best mAP@R improved from 0.8499 to 0.8520
[09-20 10:33] - Epoch 12 | lr 6.64e-04
[09-20 10:41] - Train loss: 18.4971 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:41] - Val   loss: 99.6325 | Acc@1: 0.8889 | mAP@10: 0.8739 | target: 0.8814 | mAP@R: 0.8495
[09-20 10:41] - Epoch 13 | lr 6.30e-04
[09-20 10:50] - Train loss: 18.1796 | Acc@1: 0.0000 | mAP@10: 0.0000 | target: 0.0000 | mAP@R: 0.0000
[09-20 10:50] - Val   loss: 99.6563 | Acc@1: 0.9004 | mAP@10: 0.8777 | target: 0.8890 | mAP@R: 0.8510
[09-20 10:50] - Epoch 13: best target improved from 0.8869 to 0.8890
[09-20 10:50] - Epoch 14 | lr 5.96e-04
