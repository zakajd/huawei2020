[09-16 11:00] - Parameters used for training: Namespace(arch='genet_normal', augmentation='light', batch_size=96, config_file='configs/genet_normal.yaml', criterion='arcface', criterion_params={'out_features': 3097, 's': 64.0, 'm': 0.3}, debug=False, ema_decay=0.0, embedding_size=512, model_params={}, name='genet_normal_384_arcface_fp16_gem_light', optim='adamw', outdir='logs/genet_normal_384_arcface_fp16_gem_light', phases=[{'ep': [0, 50], 'lr': [0.1, 1e-05]}], pooling='gem', resume='', root='data/interim', seed=42, size=384, tta=False, use_fp16=True, val_frequency=1, val_size=768, weight_decay=1e-05, workers=8)
[09-16 11:00] - Start training
[09-16 11:00] - Model size: 19.89M
[09-16 11:00] - Loss for this run is: AdditiveAngularMarginLoss(
  (criterion): CrossEntropyLoss()
)
[09-16 11:00] - Val size: 13761
[09-16 11:00] - Train size: 68805
[09-16 11:00] - Start phase #1 from epoch 0 to epoch 50: {'ep': [0, 50], 'lr': [0.1, 1e-05], 'mom': [], 'mode': 'linear'}
[09-16 11:00] - Epoch 1 | lr 0.00e+00
[09-16 11:03] - 
TimeMeter profiling. Data time: 7.06E-04s. Model time: 2.48E-01s 

[09-16 11:05] - Train loss: 28.0303 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:05] - Val   loss: 27.4266 | Acc_@1: 0.1631 | mAP@10: 0.2079 | .5Acc+.5mAP: 0.1855 | CMC@10: 0.4059
[09-16 11:05] - Epoch  1: best loss improved from inf to 27.4266
[09-16 11:05] - Epoch 2 | lr 9.86e-02
[09-16 11:09] - Train loss: 25.6996 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:09] - Val   loss: 26.6146 | Acc_@1: 0.4253 | mAP@10: 0.4475 | .5Acc+.5mAP: 0.4364 | CMC@10: 0.6734
[09-16 11:09] - Epoch  2: best loss improved from 27.4266 to 26.6146
[09-16 11:09] - Epoch 3 | lr 9.66e-02
[09-16 11:14] - Train loss: 22.5723 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:14] - Val   loss: 26.6324 | Acc_@1: 0.4756 | mAP@10: 0.4894 | .5Acc+.5mAP: 0.4825 | CMC@10: 0.7098
[09-16 11:14] - Epoch 4 | lr 9.46e-02
[09-16 11:19] - Train loss: 19.3539 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:19] - Val   loss: 24.8404 | Acc_@1: 0.6454 | mAP@10: 0.6499 | .5Acc+.5mAP: 0.6477 | CMC@10: 0.8457
[09-16 11:19] - Epoch  4: best loss improved from 26.6146 to 24.8404
[09-16 11:19] - Epoch 5 | lr 9.26e-02
[09-16 11:23] - Train loss: 16.3351 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:23] - Val   loss: 23.7217 | Acc_@1: 0.7311 | mAP@10: 0.7277 | .5Acc+.5mAP: 0.7294 | CMC@10: 0.8832
[09-16 11:23] - Epoch  5: best loss improved from 24.8404 to 23.7217
[09-16 11:23] - Epoch 6 | lr 9.06e-02
[09-16 11:28] - Train loss: 13.6351 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:28] - Val   loss: 23.9860 | Acc_@1: 0.6992 | mAP@10: 0.6961 | .5Acc+.5mAP: 0.6977 | CMC@10: 0.8599
[09-16 11:28] - Epoch 7 | lr 8.86e-02
[09-16 11:32] - Train loss: 11.3586 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:32] - Val   loss: 22.5496 | Acc_@1: 0.7459 | mAP@10: 0.7397 | .5Acc+.5mAP: 0.7428 | CMC@10: 0.8928
[09-16 11:32] - Epoch  7: best loss improved from 23.7217 to 22.5496
[09-16 11:32] - Epoch 8 | lr 8.66e-02
[09-16 11:37] - Train loss: 9.5676 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:37] - Val   loss: 20.7320 | Acc_@1: 0.7905 | mAP@10: 0.7841 | .5Acc+.5mAP: 0.7873 | CMC@10: 0.9197
[09-16 11:37] - Epoch  8: best loss improved from 22.5496 to 20.7320
[09-16 11:37] - Epoch 9 | lr 8.46e-02
[09-16 11:43] - Train loss: 8.0877 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:43] - Val   loss: 19.9834 | Acc_@1: 0.7916 | mAP@10: 0.7882 | .5Acc+.5mAP: 0.7899 | CMC@10: 0.9112
[09-16 11:43] - Epoch  9: best loss improved from 20.7320 to 19.9834
[09-16 11:43] - Epoch 10 | lr 8.26e-02
[09-16 11:48] - Train loss: 6.9358 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:48] - Val   loss: 20.0579 | Acc_@1: 0.7785 | mAP@10: 0.7691 | .5Acc+.5mAP: 0.7738 | CMC@10: 0.9023
[09-16 11:48] - Epoch 11 | lr 8.06e-02
[09-16 11:52] - Train loss: 5.9542 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:52] - Val   loss: 22.3705 | Acc_@1: 0.7403 | mAP@10: 0.7366 | .5Acc+.5mAP: 0.7384 | CMC@10: 0.8797
[09-16 11:52] - Epoch 12 | lr 7.86e-02
[09-16 11:57] - Train loss: 5.1467 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 11:57] - Val   loss: 19.5716 | Acc_@1: 0.7781 | mAP@10: 0.7699 | .5Acc+.5mAP: 0.7740 | CMC@10: 0.8910
[09-16 11:57] - Epoch 12: best loss improved from 19.9834 to 19.5716
[09-16 11:57] - Epoch 13 | lr 7.66e-02
[09-16 12:01] - Train loss: 4.4965 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:01] - Val   loss: 17.8268 | Acc_@1: 0.8401 | mAP@10: 0.8329 | .5Acc+.5mAP: 0.8365 | CMC@10: 0.9413
[09-16 12:01] - Epoch 13: best loss improved from 19.5716 to 17.8268
[09-16 12:01] - Epoch 14 | lr 7.46e-02
[09-16 12:06] - Train loss: 3.9444 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:06] - Val   loss: 18.3189 | Acc_@1: 0.8185 | mAP@10: 0.8134 | .5Acc+.5mAP: 0.8159 | CMC@10: 0.9218
[09-16 12:06] - Epoch 15 | lr 7.26e-02
[09-16 12:10] - Train loss: 3.4671 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:10] - Val   loss: 17.4392 | Acc_@1: 0.8309 | mAP@10: 0.8246 | .5Acc+.5mAP: 0.8277 | CMC@10: 0.9321
[09-16 12:10] - Epoch 15: best loss improved from 17.8268 to 17.4392
[09-16 12:10] - Epoch 16 | lr 7.06e-02
[09-16 12:15] - Train loss: 3.0607 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:15] - Val   loss: 17.2145 | Acc_@1: 0.8418 | mAP@10: 0.8359 | .5Acc+.5mAP: 0.8389 | CMC@10: 0.9388
[09-16 12:15] - Epoch 16: best loss improved from 17.4392 to 17.2145
[09-16 12:15] - Epoch 17 | lr 6.86e-02
[09-16 12:19] - Train loss: 2.7669 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:19] - Val   loss: 17.0682 | Acc_@1: 0.8485 | mAP@10: 0.8438 | .5Acc+.5mAP: 0.8462 | CMC@10: 0.9434
[09-16 12:19] - Epoch 17: best loss improved from 17.2145 to 17.0682
[09-16 12:19] - Epoch 18 | lr 6.66e-02
[09-16 12:24] - Train loss: 2.4177 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:24] - Val   loss: 17.5260 | Acc_@1: 0.8266 | mAP@10: 0.8206 | .5Acc+.5mAP: 0.8236 | CMC@10: 0.9349
[09-16 12:24] - Epoch 19 | lr 6.46e-02
[09-16 12:28] - Train loss: 2.1739 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:28] - Val   loss: 17.2191 | Acc_@1: 0.8259 | mAP@10: 0.8186 | .5Acc+.5mAP: 0.8223 | CMC@10: 0.9268
[09-16 12:28] - Epoch 20 | lr 6.26e-02
[09-16 12:32] - Train loss: 1.9527 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:32] - Val   loss: 16.9233 | Acc_@1: 0.8284 | mAP@10: 0.8258 | .5Acc+.5mAP: 0.8271 | CMC@10: 0.9292
[09-16 12:32] - Epoch 20: best loss improved from 17.0682 to 16.9233
[09-16 12:32] - Epoch 21 | lr 6.06e-02
[09-16 12:37] - Train loss: 1.7568 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:37] - Val   loss: 17.4449 | Acc_@1: 0.8181 | mAP@10: 0.8158 | .5Acc+.5mAP: 0.8170 | CMC@10: 0.9285
[09-16 12:37] - Epoch 22 | lr 5.86e-02
[09-16 12:41] - Train loss: 1.5686 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:41] - Val   loss: 16.3939 | Acc_@1: 0.8517 | mAP@10: 0.8431 | .5Acc+.5mAP: 0.8474 | CMC@10: 0.9452
[09-16 12:41] - Epoch 22: best loss improved from 16.9233 to 16.3939
[09-16 12:41] - Epoch 23 | lr 5.66e-02
[09-16 12:46] - Train loss: 1.4181 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:46] - Val   loss: 17.1540 | Acc_@1: 0.8185 | mAP@10: 0.8126 | .5Acc+.5mAP: 0.8155 | CMC@10: 0.9257
[09-16 12:46] - Epoch 24 | lr 5.46e-02
[09-16 12:50] - Train loss: 1.2943 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:50] - Val   loss: 16.1306 | Acc_@1: 0.8507 | mAP@10: 0.8441 | .5Acc+.5mAP: 0.8474 | CMC@10: 0.9444
[09-16 12:50] - Epoch 24: best loss improved from 16.3939 to 16.1306
[09-16 12:50] - Epoch 25 | lr 5.26e-02
[09-16 12:55] - Train loss: 1.1673 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:55] - Val   loss: 16.2257 | Acc_@1: 0.8517 | mAP@10: 0.8452 | .5Acc+.5mAP: 0.8485 | CMC@10: 0.9466
[09-16 12:55] - Epoch 26 | lr 5.06e-02
[09-16 12:59] - Train loss: 1.0627 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 12:59] - Val   loss: 15.8399 | Acc_@1: 0.8655 | mAP@10: 0.8587 | .5Acc+.5mAP: 0.8621 | CMC@10: 0.9544
[09-16 12:59] - Epoch 26: best loss improved from 16.1306 to 15.8399
[09-16 12:59] - Epoch 27 | lr 4.86e-02
[09-16 13:04] - Train loss: 0.9848 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:04] - Val   loss: 16.3028 | Acc_@1: 0.8475 | mAP@10: 0.8384 | .5Acc+.5mAP: 0.8429 | CMC@10: 0.9406
[09-16 13:04] - Epoch 28 | lr 4.66e-02
[09-16 13:08] - Train loss: 0.9133 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:08] - Val   loss: 16.1440 | Acc_@1: 0.8546 | mAP@10: 0.8470 | .5Acc+.5mAP: 0.8508 | CMC@10: 0.9444
[09-16 13:08] - Epoch 29 | lr 4.46e-02
[09-16 13:13] - Train loss: 0.8517 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:13] - Val   loss: 16.3840 | Acc_@1: 0.8248 | mAP@10: 0.8212 | .5Acc+.5mAP: 0.8230 | CMC@10: 0.9299
[09-16 13:13] - Epoch 30 | lr 4.26e-02
[09-16 13:17] - Train loss: 0.7918 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:17] - Val   loss: 16.1479 | Acc_@1: 0.8422 | mAP@10: 0.8341 | .5Acc+.5mAP: 0.8381 | CMC@10: 0.9402
[09-16 13:17] - Epoch 31 | lr 4.06e-02
[09-16 13:22] - Train loss: 0.7508 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:22] - Val   loss: 16.0646 | Acc_@1: 0.8404 | mAP@10: 0.8360 | .5Acc+.5mAP: 0.8382 | CMC@10: 0.9391
[09-16 13:22] - Epoch 32 | lr 3.86e-02
[09-16 13:26] - Train loss: 0.7099 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:26] - Val   loss: 15.6322 | Acc_@1: 0.8503 | mAP@10: 0.8435 | .5Acc+.5mAP: 0.8469 | CMC@10: 0.9409
[09-16 13:26] - Epoch 32: best loss improved from 15.8399 to 15.6322
[09-16 13:26] - Epoch 33 | lr 3.66e-02
[09-16 13:31] - Train loss: 0.6700 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:31] - Val   loss: 15.5968 | Acc_@1: 0.8694 | mAP@10: 0.8593 | .5Acc+.5mAP: 0.8643 | CMC@10: 0.9529
[09-16 13:31] - Epoch 33: best loss improved from 15.6322 to 15.5968
[09-16 13:31] - Epoch 34 | lr 3.46e-02
[09-16 13:35] - Train loss: 0.6607 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:35] - Val   loss: 15.7254 | Acc_@1: 0.8351 | mAP@10: 0.8320 | .5Acc+.5mAP: 0.8335 | CMC@10: 0.9331
[09-16 13:35] - Epoch 35 | lr 3.26e-02
[09-16 13:40] - Train loss: 0.6349 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:40] - Val   loss: 15.2465 | Acc_@1: 0.8641 | mAP@10: 0.8590 | .5Acc+.5mAP: 0.8615 | CMC@10: 0.9561
[09-16 13:40] - Epoch 35: best loss improved from 15.5968 to 15.2465
[09-16 13:40] - Epoch 36 | lr 3.06e-02
[09-16 13:44] - Train loss: 0.6117 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:44] - Val   loss: 15.4888 | Acc_@1: 0.8528 | mAP@10: 0.8472 | .5Acc+.5mAP: 0.8500 | CMC@10: 0.9476
[09-16 13:44] - Epoch 37 | lr 2.86e-02
[09-16 13:49] - Train loss: 0.5907 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:49] - Val   loss: 15.4966 | Acc_@1: 0.8521 | mAP@10: 0.8463 | .5Acc+.5mAP: 0.8492 | CMC@10: 0.9423
[09-16 13:49] - Epoch 38 | lr 2.66e-02
[09-16 13:54] - Train loss: 0.5775 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:54] - Val   loss: 15.1402 | Acc_@1: 0.8666 | mAP@10: 0.8584 | .5Acc+.5mAP: 0.8625 | CMC@10: 0.9536
[09-16 13:54] - Epoch 38: best loss improved from 15.2465 to 15.1402
[09-16 13:54] - Epoch 39 | lr 2.46e-02
[09-16 13:58] - Train loss: 0.5574 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 13:58] - Val   loss: 15.1298 | Acc_@1: 0.8666 | mAP@10: 0.8606 | .5Acc+.5mAP: 0.8636 | CMC@10: 0.9519
[09-16 13:58] - Epoch 39: best loss improved from 15.1402 to 15.1298
[09-16 13:58] - Epoch 40 | lr 2.26e-02
[09-16 14:03] - Train loss: 0.5527 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:03] - Val   loss: 15.0682 | Acc_@1: 0.8631 | mAP@10: 0.8568 | .5Acc+.5mAP: 0.8599 | CMC@10: 0.9512
[09-16 14:03] - Epoch 40: best loss improved from 15.1298 to 15.0682
[09-16 14:03] - Epoch 41 | lr 2.06e-02
[09-16 14:07] - Train loss: 0.5408 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:07] - Val   loss: 14.7885 | Acc_@1: 0.8754 | mAP@10: 0.8704 | .5Acc+.5mAP: 0.8729 | CMC@10: 0.9604
[09-16 14:07] - Epoch 41: best loss improved from 15.0682 to 14.7885
[09-16 14:07] - Epoch 42 | lr 1.86e-02
[09-16 14:12] - Train loss: 0.5405 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:12] - Val   loss: 14.5891 | Acc_@1: 0.8779 | mAP@10: 0.8728 | .5Acc+.5mAP: 0.8754 | CMC@10: 0.9625
[09-16 14:12] - Epoch 42: best loss improved from 14.7885 to 14.5891
[09-16 14:12] - Epoch 43 | lr 1.66e-02
[09-16 14:16] - Train loss: 0.5254 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:16] - Val   loss: 14.9339 | Acc_@1: 0.8549 | mAP@10: 0.8508 | .5Acc+.5mAP: 0.8529 | CMC@10: 0.9490
[09-16 14:16] - Epoch 44 | lr 1.46e-02
[09-16 14:21] - Train loss: 0.5165 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:21] - Val   loss: 14.8369 | Acc_@1: 0.8588 | mAP@10: 0.8546 | .5Acc+.5mAP: 0.8567 | CMC@10: 0.9494
[09-16 14:21] - Epoch 45 | lr 1.26e-02
[09-16 14:25] - Train loss: 0.5127 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:25] - Val   loss: 14.5925 | Acc_@1: 0.8730 | mAP@10: 0.8670 | .5Acc+.5mAP: 0.8700 | CMC@10: 0.9558
[09-16 14:25] - Epoch 46 | lr 1.06e-02
[09-16 14:30] - Train loss: 0.5100 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:30] - Val   loss: 14.4976 | Acc_@1: 0.8666 | mAP@10: 0.8640 | .5Acc+.5mAP: 0.8653 | CMC@10: 0.9579
[09-16 14:30] - Epoch 46: best loss improved from 14.5891 to 14.4976
[09-16 14:30] - Epoch 47 | lr 8.62e-03
[09-16 14:34] - Train loss: 0.5086 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:34] - Val   loss: 14.6215 | Acc_@1: 0.8652 | mAP@10: 0.8577 | .5Acc+.5mAP: 0.8615 | CMC@10: 0.9551
[09-16 14:34] - Epoch 48 | lr 6.62e-03
[09-16 14:39] - Train loss: 0.4997 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:39] - Val   loss: 14.5022 | Acc_@1: 0.8719 | mAP@10: 0.8682 | .5Acc+.5mAP: 0.8701 | CMC@10: 0.9586
[09-16 14:39] - Epoch 49 | lr 4.62e-03
[09-16 14:43] - Train loss: 0.4893 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:43] - Val   loss: 14.6124 | Acc_@1: 0.8645 | mAP@10: 0.8619 | .5Acc+.5mAP: 0.8632 | CMC@10: 0.9554
[09-16 14:43] - Epoch 50 | lr 2.62e-03
[09-16 14:48] - Train loss: 0.4846 | Acc_@1: 0.0000 | mAP@10: 0.0000 | .5Acc+.5mAP: 0.0000 | CMC@10: 0.0000
[09-16 14:48] - Val   loss: 14.4663 | Acc_@1: 0.8673 | mAP@10: 0.8634 | .5Acc+.5mAP: 0.8654 | CMC@10: 0.9558
[09-16 14:48] - Epoch 50: best loss improved from 14.4976 to 14.4663
[09-16 14:48] - Loading best model from previous phase
[09-16 14:48] - Finished Training. Took: 228.24m
